{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as nnf\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from itertools import groupby\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from skimage.draw import disk\n",
    "import pandas as pd\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook offers keypoint detection as an example of an image-to-image tasks. Compared to many other tasks we worked on, we want to obtain a pixel-wise prediction here. That is, for each pixel we want to output a probability whether a certain keypoint is present there or not. We start by adapting a ResNet to produce dense (pixel-wise) output. Then this modified U-Net-style (https://arxiv.org/abs/1505.04597) ResNet will be applied to a keypoint estimation problem. The data we use is based on publicly available data from a paper by Psota et al. about multi-pig part detection: https://www.mdpi.com/1424-8220/19/4/852"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### UNet model\n",
    "\n",
    "In this exercise, we will take a `ResNet18` network and add a decoder such that the network generates an output tensor having the same spatial extent as the input. This network structure is called U-net. Since a U-net relies on saving the activations from the encoder part (in this case the resnet), it is advised to first thoroughly investigate the structure of the resnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "rn18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# look at how the input is processed within the network\n",
    "summary(rn18, input_size=(1, 3, 224, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how the modules are named\n",
    "for name, module in rn18.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Once you explored the structure, you can go ahead and start implementing the ``UNet`` module. In the first stage of the UNet, the input is processed by the ResNet to obtain a smaller, but information-rich feature map:\n",
    "* Loop through all resnet modules until \"layer4\" and successively apply them.\n",
    "* Store the original input as well as the features of the resnet after the modules named 'relu', 'layer1', 'layer2', 'layer3'. These will be used for the skip-concatenations later.\n",
    "\n",
    "2) After the encoder stage, the semantically rich features are hierarchically propagated to the original image size. For this, you need to build a ``DecoderBlock`` that will be instantiated multiple times for decoding:\n",
    "* The first decoder block takes as input the final output of the encoder (after layer4) and the last stored feature from the encoder (layer3).\n",
    "* The feature from layer4 needs to be upsampled (``nn.ConvTranspose2d``) by a factor of two since it has half the width and height of the last stored feature (check for yourself with summary!)\n",
    "* The upsampled layer4 feature should then be concatenated with the layer3 feature and further processed with two convolutions. After these convolutions, the channel size should be the normal channel size of layer3 again. So the feature shape is exactly the same as before but it has been 'enriched' by the information from layer4. *One exception*: After the last decoder block, the enriched feature should not have only three channels but rather 64. Three would not be enough to store all keypoint related information.\n",
    "* The subsequent decoder block takes the enriched layer3 feature and the next stored feature (layer2) as input, and so on until you arrive at the original image input.\n",
    "\n",
    "3) After all decoder steps have been performed you will end up with a feature map that has the same size as the image and 64 channels. \n",
    "* Process this feature map further with a 1x1 convolution to obtain 5 channels (1 for background and the other 4 for the four keypoints) that can be used to obtain probabilities for the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A DecoderBlock performs upsampling followed by convolutional layers with batch normalization and ReLU activation.\n",
    "    \n",
    "    Args:\n",
    "        input_channels1 (int): Number of input channels for the up-convolution (ConvTranspose2d).\n",
    "        input_channels2 (int): Number of input channels from the skip connection to be concatenated.\n",
    "        output_channels (int): Number of output channels after the convolutions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels1, input_channels2, output_channels):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        # up-convolution\n",
    "        \n",
    "        # further convolutions\n",
    "        pass\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderBlock.\n",
    "        \n",
    "        Args:\n",
    "            input1 (torch.Tensor): The input tensor to be upsampled.\n",
    "            input2 (torch.Tensor): The input tensor from the skip connection to be concatenated.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after upsampling and convolutional processing.\n",
    "        \"\"\"\n",
    "        ######### YOUR CODE HERE:\n",
    "        # up-convolution and concatenation with skip connection\n",
    "        \n",
    "        # further processing with convolutions\n",
    "        return\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net model with a ResNet backbone for feature extraction and a decoder for upsampling and keypoint prediction.\n",
    "\n",
    "    Args:\n",
    "        resnet (nn.Module): A ResNet model to be used as the encoder.\n",
    "        resnet_layers (list of str): List of layer names from the ResNet model to be used in the encoder.\n",
    "        resnet_layers_save (list of str): List of layer names from the ResNet model whose activations are to be saved for skip connections.\n",
    "        input_channels1 (list of int): List of input channel sizes for each DecoderBlock in the decoder.\n",
    "        input_channels2 (list of int): List of input channel sizes from skip connections for each DecoderBlock in the decoder.\n",
    "        output_channels (list of int): List of output channel sizes for each DecoderBlock in the decoder.\n",
    "        final_layer_channels (int): Number of output channels for the final convolutional layer, i.e. the number of keypoint heatmaps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resnet, resnet_layers=['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4'], \n",
    "                 resnet_layers_save=['relu', 'layer1', 'layer2', 'layer3'], \n",
    "                 input_channels1=[512, 256, 128, 64, 64], input_channels2=[256, 128, 64, 64, 3], output_channels=[256, 128, 64, 64, 64],\n",
    "                 final_layer_channels=5):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "\n",
    "        # resnet\n",
    "\n",
    "        # decoder layers\n",
    "            \n",
    "        # final convolution to obtain keypoint heatmaps\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the U-Net model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor, typically an image.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor, typically containing keypoint heatmaps.\n",
    "        \"\"\"\n",
    "        ######### YOUR CODE HERE:\n",
    "        # save activations of initial input and specified resnet layers\n",
    "\n",
    "        # decoder\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass dummy tensors (e.g. torch.randn) to the network to see if everything works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a trained model, we can identify which class (keypoints and background) is predicted to be most likely for a given pixel by calculating the argmax over all channels. The ``plot_pred_and_gt`` function defined here takes as arguments the model output and a ground truth keypoint map and plots the predicted and ground truth class at every pixel position. There is also a dummy ground truth keypoint map defined in the cell. The ground truth has the same width and height as the prediction but only one channel with integers representing the class at a certain image position (keypoints or background).\n",
    "\n",
    "* Plot the model output you obtained using the dummy input together with the dummy ground truth keypoints. How do you expect the prediction to look?\n",
    "* ``nnf.cross_entropy`` can be used for image-to-image tasks by applying it per-pixel. Calculate it with the output and dummy ground truth keypoints. What value would you expect? And why is the value you observe maybe a bit different?\n",
    "\n",
    "######### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_and_gt(a_pred: torch.Tensor, a_gt: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    This function plots the prediction and the ground truth tensors. The prediction tensor has 3 channels, one for each class.\n",
    "    The pixels with the highest value over all channels is plotted in the first subplot. The second subplot shows the ground truth.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(1, 2)\n",
    "    # The argmax(0) returns the highest value for each pixel over the 3 channels.\n",
    "    # The result is a tensor of shape (H, W) with H height and W width \n",
    "    # where each value represents the channel index with the highest value.\n",
    "    ax[0].imshow(a_pred.argmax(0))\n",
    "    ax[1].imshow(a_gt)\n",
    "    ax[0].set_title('Prediction')\n",
    "    ax[1].set_title('Ground Truth')\n",
    "    plt.show()\n",
    "    \n",
    "# dummy ground truth keypoint locations\n",
    "a_gt = torch.zeros(224, 384, dtype=torch.int64)\n",
    "a_gt[30:90, 110:170] = 1\n",
    "a_gt[130:190, 110:170] = 2\n",
    "a_gt[30:90, 210:270] = 3\n",
    "a_gt[130:190, 210:270] = 4\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we created multiple dummy predictions that match the ground truth a little bit better. Looking at the plots, how do you think the cross entropy loss will be for these examples? If you want, you can verify your intuition using the ``nnf.cross_entropy`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates example tensors for the predictions and the ground truth (gt).\n",
    "prediction = 100\n",
    "a_pred = -prediction * torch.ones(5, 224, 384)\n",
    "a_pred[0, ...] = prediction\n",
    "a_pred[0, 30:90, 110:170] = -prediction\n",
    "a_pred[0, 130:190, 110:170] = -prediction\n",
    "a_pred[0, 30:90, 210:270] = -prediction\n",
    "a_pred[0, 130:190, 210:270] = -prediction\n",
    "a_pred[1, 30:90, 110:170] = prediction\n",
    "a_pred[2, 130:190, 110:170] = prediction\n",
    "a_pred[3, 30:90, 210:270] = prediction\n",
    "a_pred[4, 130:190, 210:270] = prediction\n",
    "\n",
    "plot_pred_and_gt(a_pred, a_gt)\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates example tensors for the predictions and the ground truth (gt).\n",
    "prediction = 1\n",
    "a_pred = -prediction * torch.ones(5, 224, 384)\n",
    "a_pred[0, ...] = prediction\n",
    "a_pred[0, 50:110, 110:170] = -prediction\n",
    "a_pred[0, 150:210, 110:170] = -prediction\n",
    "a_pred[0, 50:110, 210:270] = -prediction\n",
    "a_pred[0, 150:210, 210:270] = -prediction\n",
    "a_pred[1, 50:110, 110:170] = prediction\n",
    "a_pred[2, 150:210, 110:170] = prediction\n",
    "a_pred[3, 50:110, 210:270] = prediction\n",
    "a_pred[4, 150:210, 210:270] = prediction\n",
    "\n",
    "plot_pred_and_gt(a_pred, a_gt)\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates example tensors for the predictions and the ground truth (gt).\n",
    "prediction = 1\n",
    "a_pred = -prediction * torch.ones(5, 224, 384)\n",
    "a_pred[0, ...] = prediction\n",
    "a_pred[0, 50:110, 150:210] = -prediction\n",
    "a_pred[0, 150:210, 150:210] = -prediction\n",
    "a_pred[0, 50:110, 250:310] = -prediction\n",
    "a_pred[0, 150:210, 250:310] = -prediction\n",
    "a_pred[1, 50:110, 150:210] = prediction\n",
    "a_pred[2, 150:210, 150:210] = prediction\n",
    "a_pred[3, 50:110, 250:310] = prediction\n",
    "a_pred[4, 150:210, 250:310] = prediction\n",
    "\n",
    "plot_pred_and_gt(a_pred, a_gt)\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The below code loads the ``annotations.json`` file and exlores its contents. Since the number of pigs and keypoints per image is variable, the annotation format (standard COCO keypoints annotation format) is a little bit more complex. ``annotations.json`` has multiple entries that contain information regarding the images and keypoints. The information can be linked since annotations are associated with an image id that they belong to. The following code explores the annotations object and performs some preprocessing steps needed to obtain the ``KeypointsDataset`` for model training. You do not need to write any code yourself here and it is also not essential to understand all the code. The code is still commented in case you want to understand what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations\n",
    "images_dir = os.path.join(raw_data_dir, '6_keypoint_detection/images')\n",
    "pig_kp = json.load(open(os.path.join(raw_data_dir, '6_keypoint_detection/annotations.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### explore keys of pig_kp dictionary\n",
    "# only the first three keys are of importance for the task at hand\n",
    "print('keys of pig_kp object:', pig_kp.keys())\n",
    "\n",
    "############### explore images entry of pig_kp\n",
    "print(f'\\nsructure of \"images\" entry of json dict: {pig_kp[\"images\"][0]}')\n",
    "print(f'number of images: {len(pig_kp[\"images\"])}')\n",
    "# the important entries for the task at hand are 'file_name', 'id', 'width', 'height'\n",
    "\n",
    "############### explore annotations entry of pig_kp\n",
    "print(f'\\nsructure of \"annotations\" entry of json dict: {pig_kp[\"annotations\"][0]}') # important entries are 'file_name', 'id', 'width', 'height'\n",
    "print(f'number of annotated pigs: {len(pig_kp[\"annotations\"])}')\n",
    "# each annotated pig is associated with an image through the 'image_id' entry\n",
    "# it also comes with a category_id, which is always 1 in this case since we only consider pigs\n",
    "# Most importantly, the 'keypoints' entry contains the four keypoints of the pig. \n",
    "# The standard encoding is used, i.e. [x1, y1, v1, x2, y2, v2, ...] where vi is the visibility of the ith keypoint (2: visible, 1: occluded, 0: not visible, but 1 is not used in this dataset).\n",
    "# The order of they keypoints can be found in the 'categories' entry of pig_kp\n",
    "\n",
    "############### explore categories entry of pig_kp\n",
    "print(f'\\nsructure of \"categories\" entry of json dict: {pig_kp[\"categories\"]}') # important entries are 'file_name', 'id', 'width', 'height'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### creation of helper objects for later use\n",
    "\n",
    "# filter out empty annotations (some images do not have annotated keypoints)\n",
    "anns = pig_kp['annotations']\n",
    "anns = [a for a in anns if len([v for v in a['keypoints'][2::3] if v==2]) > 0]\n",
    "\n",
    "# create annotations by image_id and annotation id\n",
    "anns = sorted(anns, key=lambda x:x['image_id'])\n",
    "\n",
    "# dictionary with image_id as key and list of corresponding annotation ids as value\n",
    "anns_by_image = {k: [a['id'] for a in v] for k,v in groupby(anns, lambda x: x['image_id'])}\n",
    "\n",
    "# dictionary with annotation id as key and corresponding annotation information as value\n",
    "anns_by_id = {a['id']: a for a in anns}\n",
    "\n",
    "# dictionary with image id as key and corresponding image information as value\n",
    "images_by_id = {img['id']: img for img in pig_kp['images']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(anns_by_image, anns_by_id, images_by_id, images_dir, img_ids):\n",
    "    \"\"\"\n",
    "    Generate labeled samples for keypoint detection.\n",
    "\n",
    "    This function processes images and their corresponding keypoint annotations to create a list of samples. \n",
    "    Each sample consists of an image and a label tensor, where the label tensor contains integer values \n",
    "    where different integer values represents the presence of different keypoints.\n",
    "\n",
    "    Args:\n",
    "        anns_by_image (dict): A dictionary mapping image IDs to lists of annotation IDs associated with that image.\n",
    "        anns_by_id (dict): A dictionary mapping annotation IDs to annotation data, which includes keypoints information.\n",
    "        images_by_id (dict): A dictionary mapping image IDs to image metadata, including file name, width, and height.\n",
    "        images_dir (str): The directory where the images are stored.\n",
    "        img_ids (set): A set of image IDs that should be processed. Images not in this set will be skipped.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            - img (PIL.Image.Image): The loaded image.\n",
    "            - labels (PIL.Image.Image): A label tensor image with the same dimensions as the input image. \n",
    "              The tensor contains integer values where each keypoint type is represented \n",
    "              by a unique integer value (1, 2, 3, ...). The value is 0 where no keypoints are present.\n",
    "    \n",
    "    Notes:\n",
    "        - The function assumes that the `keypoints` field in each annotation is a list where each keypoint is \n",
    "          represented by three values: [x, y, v], where `x` and `y` are coordinates and `v` is the visibility flag.\n",
    "        - Keypoints with a visibility flag `v` equal to 2 are used to label the corresponding locations in the label tensor.\n",
    "        - The label tensor is filled using small disks centered at the keypoint locations.\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = []\n",
    "    for img_id, ann_ids in anns_by_image.items():\n",
    "        # skip images that are not in the specified img_ids\n",
    "        if not img_id in img_ids:\n",
    "            continue\n",
    "        \n",
    "        # load image\n",
    "        filename = os.path.join(images_dir, images_by_id[img_id]['file_name'])\n",
    "        img = Image.open(filename)\n",
    "        \n",
    "        # get width and height of image\n",
    "        img_w, img_h = images_by_id[img_id]['width'], images_by_id[img_id]['height']\n",
    "\n",
    "        # create labels tensor that has same width and height as image and fill it wihth zeros initially\n",
    "        labels = np.zeros((img_h, img_w), 'uint8')    \n",
    "        \n",
    "        # get keypoints that are associated with the image. Fill the labels tensor with the respective integer values (1 for first keypoint, 2 for second, ...)\n",
    "        for i, ann_id in enumerate(ann_ids):\n",
    "            ann = anns_by_id[ann_id]\n",
    "            kp = np.array(ann['keypoints']).astype('float32')\n",
    "            x = kp[0::3]\n",
    "            y = kp[1::3]\n",
    "            v = kp[2::3]\n",
    "            \n",
    "            for j in range(4):\n",
    "                if v[j] == 2:\n",
    "                    rr, cc = disk((y[j], x[j]), 3, shape=(img_h, img_w))\n",
    "                    labels[rr, cc] = j+1   \n",
    "                    \n",
    "        # add image and labels to samples list as a tuple\n",
    "        labels = Image.fromarray(labels.astype('int8'))\n",
    "        samples += [(img, labels)]\n",
    "    return samples\n",
    "\n",
    "\n",
    "class KeypointsDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        # directly store all samples in working memory (This is different from the previous exercises, where we loaded the samples on-the-fly)\n",
    "        self.samples = samples\n",
    "        self.img_transform = transforms.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), # standard normalization for models pretrained on ImageNet\n",
    "                                        std=torch.tensor([0.229, 0.224, 0.225])\n",
    "                                        )\n",
    "        \n",
    "    def transform(self, img, labels):\n",
    "        # Convert to tensor and normalize image\n",
    "        img = transforms.ToTensor()(img)\n",
    "        labels = transforms.ToTensor()(labels)\n",
    "        img = self.img_transform(img)\n",
    "        return img, labels.squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        img, labels = sample\n",
    "        img, labels = self.transform(img, labels)\n",
    "        return img, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "random.seed(1)\n",
    "\n",
    "# get img ids for training and validation\n",
    "all_img_ids = list(images_by_id.keys())\n",
    "\n",
    "# randomly shuffle img ids and divide them into training and validation img ids\n",
    "random.shuffle(all_img_ids)\n",
    "train_inds = all_img_ids[:int(0.9*len(all_img_ids))]\n",
    "val_inds = all_img_ids[int(0.9*len(all_img_ids)):]\n",
    "\n",
    "# get corresponding samples for training and validation\n",
    "samples_train = get_samples(anns_by_image, anns_by_id, images_by_id, images_dir, train_inds)\n",
    "samples_val = get_samples(anns_by_image, anns_by_id, images_by_id, images_dir, val_inds)\n",
    "\n",
    "# create dataset and dataloaders with extracted samples\n",
    "trainset = KeypointsDataset(samples_train)\n",
    "valset = KeypointsDataset(samples_val)\n",
    "trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f'length of validation set: {len(trainset)}')\n",
    "print(f'length of validation set: {len(valset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some examples from the keypoints dataset. The label tensor from the dataset has the same shape as the image but only one channel with integers representing the different classes (keypoints and background). The keypoints were enlarged to a small circle (see image) around the keypoint positions from the annotation. \n",
    "* Why is that important for training?\n",
    "\n",
    "######### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some samples from KeypointsDataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img, labels = trainset[i]\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    ax.imshow(img.permute([1, 2, 0]))\n",
    "    \n",
    "    # plot keypoints (places where the labels tensor is not zero in the color of the respective keypoint)\n",
    "    colors = ['red', 'teal', 'blue', 'yellow']\n",
    "    for j in range(1, 5):\n",
    "        y, x = torch.where(labels == j)\n",
    "        ax.scatter(x.numpy(), y.numpy(), c=colors[j-1], s=0.1)  # Plot keypoints\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model can, once again, be done using almost exactly the same functions as in the previous exercises! That's why the code is already provided. You can directly start training. Do not forget to monitor the training loss and look for potential issues there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trainloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch, in trainloader:\n",
    "        x_batch = x_batch.float().to(device)\n",
    "        y_batch = y_batch.long().to(device)\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = nnf.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "def validate(model, valloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valloader:\n",
    "            x_batch = x_batch.float().to(device)\n",
    "            y_batch = y_batch.long().to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            loss = nnf.cross_entropy(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The model is running on {device}.\")\n",
    "\n",
    "# training parameters\n",
    "epochs = 12\n",
    "lr = 0.001\n",
    "print_interval = 1\n",
    "\n",
    "# save best model state dict\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '6_keypoint_detection')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best.pth')\n",
    "save_path_metrics = os.path.join(save_dir_state_dict, 'metrics.pkl')\n",
    "\n",
    "# instantiate model and optimizer\n",
    "rn18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model = UNet(rn18, final_layer_channels=5).cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train_one_epoch(model, trainloader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = validate(model, valloader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "        min_val_loss = val_loss\n",
    "    \n",
    "    if epoch % print_interval == 0:\n",
    "        print(f'Epoch {epoch} - train loss: {train_loss:.3f} - val loss: {val_loss:.3f} - lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "        \n",
    "    metrics = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    metrics.to_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)\n",
    "\n",
    "####################### plot losses\n",
    "num_metrics = len(results['train_loss'])\n",
    "plt.plot(np.arange(1, num_metrics + 1), results['train_loss'], c='blue', label='Training Loss')\n",
    "plt.plot(np.arange(1, num_metrics + 1), results['val_loss'], c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(results['val_loss'])\n",
    "plt.plot(index+1, results['val_loss'][index], 'kx', label='Min Validation Loss', markersize=12)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a model that is able to predict keypoint probabilities. However, right now we get entire areas with high keypoint probabilities since this is what we trained the model to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize keypoints\n",
    "model = UNet(rn18, final_layer_channels=5).cuda()\n",
    "model.load_state_dict(torch.load(save_path_state_dict))\n",
    "model.eval()  \n",
    "\n",
    "img, _ = valset[0]\n",
    "with torch.no_grad():\n",
    "    logits = model(img.unsqueeze(0).cuda())\n",
    "\n",
    "# plot image\n",
    "img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "img = img + torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "# overlay predictions for one keypoint channel\n",
    "logits = logits.squeeze().cpu()\n",
    "probabilities = torch.sigmoid(logits)\n",
    "x, y = torch.where(probabilities[3] > 0.5)\n",
    "plt.scatter(y, x, s=0.1, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to postprocess these predictions to obtain actual keypoints (i.e. singular xy coordinates) which could then be used for other downstream analyses. Your task is to write a ``get_keypoints`` function that converts keypoint heatmaps into singular keypoints. You can take the following hints as guidance, but feel free to come up with your own idea.\n",
    "\n",
    "* It takes as input the predicted keypoint probabilities of shape ``5x224x384`` and outputs a map of shape ``224x384`` that contains zeros almost everywhere except for the singular keypoint positions. At these positions there should be integers corresponding to the respective keypoint. \n",
    "* Keypoints should fulfil two properties: (1) They are the argmax across all 5 channels and (2) There are no xy positions with higher probabilities nearby, i.e. they are a local maximum within the same channel.\n",
    "* To determine (2) you can use ``nnf.max_pool2d`` and compare the pooled probabilities with the unpooled probabilities.\n",
    "* The function argument 'min_distance' represents the minimum distance between two local maxima. Think about the relationship between 'min_distance' and the kernel_size you need to use with ``nnf.max_pool2d``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(probabilities, min_distance=4):\n",
    "    \"\"\"\n",
    "    Identify keypoints from a probability map.\n",
    "\n",
    "    This function processes a probability tensor to extract keypoints. \n",
    "    A keypoint is defined as a point that fulfils two criteria: \n",
    "    (1) is a local maximum in its channel and (2) must have the \n",
    "    highest probability in the respective location across all channels.\n",
    "\n",
    "    Args:\n",
    "        probabilities (pytorch tensor): A 3D PyTorch tensor of shape [channels, width, height] representing \n",
    "                                        probability maps for each keypoint category.\n",
    "        min_distance (int, optional): The minimum distance (in pixels) between two maxima detections. \n",
    "                                      This determines 'how local' local maxima are.\n",
    "                                      Default is 4 pixels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 2D tensor of the same width and height as the input probabilities, where each \n",
    "                      pixel value represents a detected keypoint category (1-4). Pixels with no detected \n",
    "                      keypoints are assigned a value of 0.\n",
    "\n",
    "    \"\"\"\n",
    "    ######### YOUR CODE HERE:\n",
    "    \n",
    "    # 1) calculate argmax of probabilities\n",
    "    \n",
    "    # 2) calculate local maxima of probabilities\n",
    "    \n",
    "    # 3) obtain keypoints for all keypoint categories (1-4) by checking whether they fulfil the two conditions\n",
    "    \n",
    "    # dummy return\n",
    "    dummy_keypoints =  torch.randint(0, 5, (224, 384))\n",
    "    dummy_keypoints[torch.rand((224, 384)) > 0.01] = 0\n",
    "    keypoints = dummy_keypoints\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ``get_keypoints`` function is finished, you can directly plot the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize keypoints\n",
    "model = UNet(rn18, final_layer_channels=5).cuda()\n",
    "model.load_state_dict(torch.load(save_path_state_dict))\n",
    "model.eval()  \n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(30, 12))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # get predictions\n",
    "    img, _ = valset[i]\n",
    "    with torch.no_grad():\n",
    "        logits = model(img.unsqueeze(0).cuda())\n",
    "    \n",
    "    # calculate probabilities with predictions\n",
    "    logits = logits.squeeze().cpu()\n",
    "    probabilities = torch.sigmoid(logits) # could be left out without changing the result, but it is more intuitive to work with probabilities than with logits\n",
    "    \n",
    "    # get keypoints\n",
    "    keypoints = get_keypoints(probabilities)\n",
    "\n",
    "    # plot image together with keypoints\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "    img = img + torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "    ax.imshow(img.permute([1, 2, 0]))\n",
    "    \n",
    "    colors = ['red', 'teal', 'blue', 'yellow']\n",
    "    for j in range(1, 5):\n",
    "        y, x = torch.where(keypoints == j)\n",
    "        ax.scatter(x.cpu().numpy(), y.cpu().numpy(), c=colors[j-1], s=50, marker='x')  # Plot keypoints\n",
    "\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "* Try out different decoder sizes (e.g. channel sizes or number of convolution layers). Does it have an influence on the prediction performance?\n",
    "* You can also add residual connections in the decoding steps! In fact, the 'Res-Unet' is a popular modern architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
