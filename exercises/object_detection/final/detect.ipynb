{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/tasks/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5, 6, 7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image_id', 'image_name'],\n",
      "    num_rows: 873\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image_id', 'image_name'],\n",
      "    num_rows: 98\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "base_dir = '/usr/users/henrich1/exercises_summer_school/data/object_detection'\n",
    "train_path = os.path.join(base_dir, 'train.csv')\n",
    "val_path = os.path.join(base_dir, 'val.csv')\n",
    "data_files = {\n",
    "    'train': train_path,\n",
    "    'val': val_path\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "print(dataset['train'])\n",
    "print(dataset['val'])\n",
    "\n",
    "id2label = {0: 'pig'}\n",
    "label2id = {'pig': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, YolosImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import numpy as np\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        # albumentations.Resize(400, 640),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.RandomBrightnessContrast(p=0.5),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(len(category)):\n",
    "\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0, # no background class\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# transforming a batch # HIER WEITER\n",
    "def transform_aug_ann(examples, base_dir):\n",
    "    images_dir = os.path.join(base_dir, 'images')\n",
    "    labels_dir = os.path.join(base_dir, 'labels')\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, areas, categories = [], [], [], []\n",
    "    \n",
    "    for image_name in examples[\"image_name\"]:\n",
    "        image = Image.open(os.path.join(images_dir, image_name + '.jpg'))\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        \n",
    "        bbox = np.loadtxt(os.path.join(labels_dir, image_name + '.txt'))\n",
    "        if bbox.ndim == 1:\n",
    "            bbox = bbox[None, :]\n",
    "        category = [0 for _ in range(len(bbox))] # only one class\n",
    "        out = transform(image=image, bboxes=bbox, category=category)\n",
    "        area = np.array(out[\"bboxes\"])[:, 2] * np.array(out[\"bboxes\"])[:, 3]\n",
    "\n",
    "        areas.append(area)\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, areas, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].with_transform(lambda examples: transform_aug_ann(examples, base_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.1597,  0.0227, -0.0458,  ...,  0.5364,  0.5707,  0.5536],\n",
       "          [-0.3027, -0.1657,  0.0569,  ...,  0.3994,  0.6563,  0.8447],\n",
       "          [ 0.1597,  0.2111,  0.2796,  ...,  0.4508,  0.6049,  0.6734],\n",
       "          ...,\n",
       "          [-1.1932, -1.1932, -1.1932,  ..., -0.7308, -1.0390, -0.9020],\n",
       "          [-1.2103, -1.2103, -1.2103,  ..., -0.7650, -0.9020, -0.9363],\n",
       "          [-1.2103, -1.2103, -1.2103,  ..., -0.8507, -0.7993, -1.0048]],\n",
       " \n",
       "         [[ 0.2752,  0.1352,  0.0651,  ...,  0.1001,  0.0301, -0.0574],\n",
       "          [-0.1975, -0.0574,  0.1527,  ..., -0.0224,  0.1001,  0.2227],\n",
       "          [ 0.2752,  0.3102,  0.3978,  ...,  0.0476,  0.0651,  0.1001],\n",
       "          ...,\n",
       "          [-0.7402, -0.7402, -0.7402,  ..., -0.5301, -0.8803, -0.7927],\n",
       "          [-0.7752, -0.7752, -0.7752,  ..., -0.5651, -0.7402, -0.8277],\n",
       "          [-0.7752, -0.7752, -0.7752,  ..., -0.6702, -0.6527, -0.8978]],\n",
       " \n",
       "         [[-0.1661, -0.3055, -0.3927,  ...,  0.0431, -0.0092, -0.0615],\n",
       "          [-0.6367, -0.4973, -0.2881,  ..., -0.0092,  0.1302,  0.2696],\n",
       "          [-0.1661, -0.1312, -0.0441,  ...,  0.0431,  0.0953,  0.1302],\n",
       "          ...,\n",
       "          [-0.5844, -0.5844, -0.5844,  ..., -0.5495, -0.8981, -0.7936],\n",
       "          [-0.6018, -0.6018, -0.6018,  ..., -0.5844, -0.7587, -0.8284],\n",
       "          [-0.6018, -0.6018, -0.6018,  ..., -0.6715, -0.6541, -0.9156]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': {'size': tensor([ 800, 1280]), 'image_id': tensor([11]), 'class_labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]), 'boxes': tensor([[0.3852, 0.3819, 0.1063, 0.1163],\n",
       "         [0.3105, 0.2025, 0.1305, 0.0850],\n",
       "         [0.4719, 0.4381, 0.0359, 0.1887],\n",
       "         [0.3371, 0.6263, 0.0477, 0.0950],\n",
       "         [0.0238, 0.6156, 0.0461, 0.1312],\n",
       "         [0.0184, 0.4750, 0.0352, 0.0700],\n",
       "         [0.8891, 0.2600, 0.0766, 0.1975],\n",
       "         [0.2520, 0.4956, 0.1227, 0.1737],\n",
       "         [0.6410, 0.6044, 0.1086, 0.1538],\n",
       "         [0.3645, 0.3181, 0.0727, 0.1037],\n",
       "         [0.9172, 0.6513, 0.0672, 0.1700],\n",
       "         [0.4555, 0.2688, 0.1125, 0.1625],\n",
       "         [0.3934, 0.5219, 0.0633, 0.1912],\n",
       "         [0.6687, 0.5300, 0.1547, 0.1125],\n",
       "         [0.1367, 0.6206, 0.0453, 0.1612],\n",
       "         [0.1793, 0.6156, 0.0602, 0.1688],\n",
       "         [0.9402, 0.4625, 0.0492, 0.2125],\n",
       "         [0.3730, 0.6037, 0.0992, 0.2200],\n",
       "         [0.9012, 0.4575, 0.0617, 0.2025],\n",
       "         [0.3000, 0.6206, 0.0875, 0.1138],\n",
       "         [0.6613, 0.4081, 0.1430, 0.1163],\n",
       "         [0.0742, 0.2550, 0.0516, 0.0900],\n",
       "         [0.0371, 0.3325, 0.0617, 0.0750],\n",
       "         [0.4219, 0.1787, 0.0953, 0.0850],\n",
       "         [0.9605, 0.6463, 0.0664, 0.1525],\n",
       "         [0.2145, 0.6712, 0.0523, 0.1250],\n",
       "         [0.7512, 0.3219, 0.1523, 0.1338],\n",
       "         [0.8371, 0.4456, 0.1023, 0.2163],\n",
       "         [0.7859, 0.5525, 0.0906, 0.1800],\n",
       "         [0.7840, 0.5006, 0.0695, 0.1587],\n",
       "         [0.7508, 0.3638, 0.1375, 0.0525]]), 'area': tensor([12648., 11356.,  6946.,  4636.,  6195.,  2520., 15484., 21823., 17097.,\n",
       "          7719., 11696., 18720., 12393., 17820.,  7482., 10395., 10710., 22352.,\n",
       "         12798., 10192., 17019.,  4752.,  4740.,  8296., 10370.,  6700., 20865.,\n",
       "         22663., 16704., 11303.,  7392.]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([ 800, 1280])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]\n",
    "# dataset['train'][0]['labels']['orig_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr-resnet-50_finetuned_pigs\",\n",
    "    per_device_train_batch_size=6,\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=200,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=20,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "path = '/usr/users/henrich1/exercises_summer_school/data/object_detection/images/cam1_120180328-194558-1522259158_frame_27590.jpg'\n",
    "image = Image.open(path)\n",
    "\n",
    "obj_detector = pipeline(\"object-detection\", model=\"/usr/users/henrich1/exercises_summer_school/exercises/object_detection/yolo_ft/checkpoint-800\")\n",
    "obj_detector(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = image_processor(images=image, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection, YolosImageProcessor\n",
    "import torch\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"/usr/users/henrich1/exercises_summer_school/exercises/object_detection/detr-resnet-50_finetuned_pigs/checkpoint-2600\").to('cuda')\n",
    "\n",
    "# image_path = '/usr/users/henrich1/exercises_summer_school/data/object_detection/images/Kamera320170324-103300-1490347980_frame_15038.jpg'\n",
    "# image_path = '/usr/users/henrich1/exercises_summer_school/data/object_detection/images/Kamera120171025-115501-1508925301_frame_16598.jpg'\n",
    "# image_path = '/usr/users/henrich1/exercises_summer_school/data/object_detection/images/Kamera320180307-184059-1520444459_cropped_00-08-43_00-10-35_frame68_missing.jpg'\n",
    "image_path = '/usr/users/henrich1/exercises_summer_school/data/object_detection/images/Kamera420170722-140519-1500725119_cropped_00-26-34_00-26-47_frame11_lowconf.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors import safe_open\n",
    "\n",
    "\n",
    "# test = torch.load('/usr/users/henrich1/exercises_summer_school/exercises/object_detection/detr-resnet-50_finetuned_pigs/checkpoint-2750/rng_state.pth')\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"/usr/users/henrich1/exercises_summer_school/exercises/object_detection/detr-resnet-50_finetuned_pigs/checkpoint-2750/model.safetensors\", framework=\"pt\", device=0) as f:\n",
    "#     for k in f.keys():\n",
    "#         tensors[k] = f.get_tensor(k)\n",
    "\n",
    "# tensors.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
