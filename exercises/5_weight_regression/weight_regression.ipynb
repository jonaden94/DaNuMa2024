{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import matplotlib.image as mpimg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will enhance the MLP architecture from the last exercise with a well-known regularization technique, namely \"dropout\". Furthermore, you will implement a convolutional neural network and demonstrate its superiority over the MLP when it comes to image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular data\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "train_weights = pd.read_csv(train_weights_path)\n",
    "val_weights = pd.read_csv(val_weights_path)\n",
    "\n",
    "# images directory\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight                       images_dir\n",
      "0    52.5  Gr_2_WG_2_900222000834743_depth\n",
      "1    37.0  Gr_2_WG_2_900222000834745_depth\n",
      "2    41.5  Gr_2_WG_2_900222000834748_depth\n",
      "3    34.0  Gr_2_WG_2_900222000834749_depth\n",
      "4    49.0  Gr_2_WG_2_900222000834750_depth\n",
      "\n",
      "\n",
      "number of training examples: 347\n",
      "number of validation examples: 148\n",
      "mean weight: 54.68155619596542\n",
      "std weight: 13.573825326246093\n",
      "min weight: 27.0\n",
      "max weight: 102.5\n"
     ]
    }
   ],
   "source": [
    "# explore csv data\n",
    "print(train_weights.head())\n",
    "print('\\n')\n",
    "print(f'number of training examples: {train_weights.shape[0]}')\n",
    "print(f'number of validation examples: {val_weights.shape[0]}')\n",
    "print(f'mean weight: {train_weights.weight.mean()}')\n",
    "print(f'std weight: {train_weights.weight.std()}')\n",
    "print(f'min weight: {train_weights.weight.min()}')\n",
    "print(f'max weight: {train_weights.weight.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frames of one pig\n",
    "index = 5\n",
    "images_one_pig_dir = os.path.join(images_dir, train_weights['images_dir'][index])\n",
    "images_one_pig = os.listdir(images_one_pig_dir)\n",
    "images_one_pig = sorted(images_one_pig, key=lambda x: int(x[:-4].split('_')[-1]))\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 10\n",
    "figsize = (20, 5)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    if i < len(images_one_pig):\n",
    "        img_path = os.path.join(images_one_pig_dir, images_one_pig[i])\n",
    "        img = mpimg.imread(img_path)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off') \n",
    "    else:\n",
    "        axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDataset(Dataset):\n",
    "    def __init__(self, weights_df_path, images_base_dir):\n",
    "        self.weights_df = pd.read_csv(weights_df_path)\n",
    "        self.images_base_dir = images_base_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # select row from dataframe and get data from it\n",
    "        info = self.weights_df.iloc[i, :]\n",
    "        weight = torch.tensor(info.weight).float()\n",
    "        images_folder = info.images_dir\n",
    "\n",
    "        # load one random image corresponding to the weighting of the selected row\n",
    "        images_dir = os.path.join(self.images_base_dir, images_folder)\n",
    "        image_name = random.choice(os.listdir(images_dir))\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # transform and return image\n",
    "        image = self.transform(image)\n",
    "        return image, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.fc.bias.data.fill_(50) # important\n",
    "\n",
    "    def forward(self, x):\n",
    "        ######### YOUR CODE HERE:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.avg_pool(x).squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class WeightResNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        ######### YOUR CODE HERE:\n",
    "        super().__init__()\n",
    "        self.resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.resnet34.fc = nn.Linear(512, 1)\n",
    "        self.resnet34.fc.bias.data.fill_(50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet34(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trainloader, optimizer, device):\n",
    "    ######### YOUR CODE HERE:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = nnf.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "\n",
    "def validate(model, valloader, device):\n",
    "    ######### YOUR CODE HERE:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nnf.mse_loss(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The model is running on {device}.\")\n",
    "\n",
    "# training parameters\n",
    "epochs = 30\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "decay_factor = 0.1\n",
    "patience = 20\n",
    "print_interval = 5\n",
    "\n",
    "# save best model state dict and metrics\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '5_weight_regression')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best.pth')\n",
    "save_path_metrics = os.path.join(save_dir_state_dict, 'metrics.pkl')\n",
    "\n",
    "# instantiate dataset and dataloader\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "trainset = WeightDataset(train_weights_path, images_dir)\n",
    "valset = WeightDataset(val_weights_path, images_dir)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)    \n",
    "\n",
    "# Initialize model, optimizer and scheduler\n",
    "model = WeightResNet34().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=decay_factor, patience=patience)\n",
    "\n",
    "# train loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train_one_epoch(model, trainloader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss = validate(model, valloader, device)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "        min_val_loss = val_loss\n",
    "        \n",
    "    if epoch % print_interval == 0:\n",
    "        print(f'Epoch {epoch} - train loss: {train_loss:.3f} - val loss: {val_loss:.3f}')\n",
    "    \n",
    "    metrics = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    metrics.to_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylim([0,25])\n",
    "\n",
    "####################### plot losses\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['train_loss'], c='blue', label='Training Loss')\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['val_loss'], c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(val_losses)\n",
    "plt.plot(index+1, val_losses[index], 'kx', label='Min Validation Loss')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "# val_weights = pd.read_csv(val_weights_path)\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "\n",
    "# dataset and trained model\n",
    "valset = WeightDataset(val_weights_path, images_dir)\n",
    "valloader = DataLoader(valset, batch_size=1)\n",
    "model = WeightResNet34()\n",
    "best_ckpt = torch.load(save_path_state_dict)\n",
    "model.load_state_dict(best_ckpt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(valloader):\n",
    "        image = image.to(device)\n",
    "        pred = model(image)\n",
    "        preds.append(pred.item())\n",
    "        targets.append(target.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(targets, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
