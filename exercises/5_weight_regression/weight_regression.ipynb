{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from torch import distributed as dist\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path as os.path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from exlib.model import WeightResNet34\n",
    "from exlib.dataset import WeightDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will enhance the MLP architecture from the last exercise with a well-known regularization technique, namely \"dropout\". Furthermore, you will implement a convolutional neural network and demonstrate its superiority over the MLP when it comes to image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular data\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "train_weights = pd.read_csv(train_weights_path)\n",
    "val_weights = pd.read_csv(val_weights_path)\n",
    "\n",
    "# images directory\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight                       images_dir\n",
      "0    52.5  Gr_2_WG_2_900222000834743_depth\n",
      "1    37.0  Gr_2_WG_2_900222000834745_depth\n",
      "2    41.5  Gr_2_WG_2_900222000834748_depth\n",
      "3    34.0  Gr_2_WG_2_900222000834749_depth\n",
      "4    49.0  Gr_2_WG_2_900222000834750_depth\n",
      "\n",
      "\n",
      "number of training examples: 347\n",
      "number of validation examples: 148\n",
      "mean weight: 54.68155619596542\n",
      "std weight: 13.573825326246093\n",
      "min weight: 27.0\n",
      "max weight: 102.5\n"
     ]
    }
   ],
   "source": [
    "# explore csv data\n",
    "print(train_weights.head())\n",
    "print('\\n')\n",
    "print(f'number of training examples: {train_weights.shape[0]}')\n",
    "print(f'number of validation examples: {val_weights.shape[0]}')\n",
    "print(f'mean weight: {train_weights.weight.mean()}')\n",
    "print(f'std weight: {train_weights.weight.std()}')\n",
    "print(f'min weight: {train_weights.weight.min()}')\n",
    "print(f'max weight: {train_weights.weight.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frames of one pig\n",
    "index = 5\n",
    "images_one_pig_dir = os.path.join(images_dir, train_weights['images_dir'][index])\n",
    "images_one_pig = os.listdir(images_one_pig_dir)\n",
    "images_one_pig = sorted(images_one_pig, key=lambda x: int(x[:-4].split('_')[-1]))\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 10\n",
    "figsize = (20, 5)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    if i < len(images_one_pig):\n",
    "        img_path = os.path.join(images_one_pig_dir, images_one_pig[i])\n",
    "        img = mpimg.imread(img_path)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off') \n",
    "    else:\n",
    "        axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDataset(Dataset):\n",
    "    def __init__(self, weights_df_path, images_base_dir):\n",
    "        self.weights_df = pd.read_csv(weights_df_path)\n",
    "        self.images_base_dir = images_base_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # select row from dataframe and get data from it\n",
    "        info = self.weights_df.iloc[i, :]\n",
    "        weight = torch.tensor(info.weight).float()\n",
    "        images_folder = info.images_dir\n",
    "\n",
    "        # load one random image corresponding to the weighting of the selected row\n",
    "        images_dir = os.path.join(self.images_base_dir, images_folder)\n",
    "        image_name = random.choice(os.listdir(images_dir))\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # transform and return image\n",
    "        image = self.transform(image)\n",
    "        return image, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_main_process():\n",
    "#     rank, _ = get_dist_info()\n",
    "#     return rank == 0\n",
    "\n",
    "\n",
    "# # RETURNS UNIQUE IDENTIFIER OF THE CURRENT PROCESS WITHIN DISTRIBUTED PROCESS GROUP (rank) AND THE NUMBER OF SUCH PROCESSES - 1 (world_size)\n",
    "# def get_dist_info():\n",
    "#     if dist.is_available() and dist.is_initialized():\n",
    "#         rank = dist.get_rank()\n",
    "#         world_size = dist.get_world_size()\n",
    "#     else:\n",
    "#         rank = 0\n",
    "#         world_size = 1\n",
    "#     return rank, world_size\n",
    "\n",
    "\n",
    "# def get_root_logger(log_file=None, log_level=logging.INFO):\n",
    "#     logger = logging.getLogger('acoustics')\n",
    "#     # if the logger has been initialized, just return it\n",
    "#     if logger.hasHandlers():\n",
    "#         return logger\n",
    "#     logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=log_level)\n",
    "#     if not is_main_process():\n",
    "#         logger.setLevel('ERROR')\n",
    "#     elif log_file is not None:\n",
    "#         file_handler = logging.FileHandler(log_file, 'w')\n",
    "#         file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "#         file_handler.setLevel(log_level)\n",
    "#         logger.addHandler(file_handler)\n",
    "\n",
    "#     return logger\n",
    "\n",
    "\n",
    "# def init_train_logger(save_directory, config=None):\n",
    "#     # Make sure previous handlers are removed\n",
    "#     for handler in logging.root.handlers[:]:\n",
    "#         logging.root.removeHandler(handler)\n",
    "\n",
    "#     os.makedirs(os.path.abspath(save_directory), exist_ok=True)\n",
    "#     timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "#     log_file = os.path.join(save_directory, f'{timestamp}.log')\n",
    "#     logger = get_root_logger(log_file=log_file)\n",
    "#     # logger.info(f'Config:\\n{args}')\n",
    "#     # shutil.copy(args.config, os.path.join(save_directory, os.path.basename(args.config)))\n",
    "#     return logger\n",
    "\n",
    "\n",
    "# def print_log(msg, logger=None, level=logging.INFO):\n",
    "#     \"\"\"Print a log message.\n",
    "#     Args:\n",
    "#         msg (str): The message to be logged.\n",
    "#         logger (logging.Logger | str | None): The logger to be used.\n",
    "#             Some special loggers are:\n",
    "#             - \"silent\": no message will be printed.\n",
    "#             - other str: the logger obtained with `get_root_logger(logger)`.\n",
    "#             - None: The `print()` method will be used to print log messages.\n",
    "#         level (int): Logging level. Only available when `logger` is a Logger\n",
    "#             object or \"root\".\n",
    "#     \"\"\"\n",
    "#     if logger is None:\n",
    "#         print(msg)\n",
    "#     elif isinstance(logger, logging.Logger):\n",
    "#         logger.log(level, msg)\n",
    "#     elif logger == 'silent':\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise TypeError(\n",
    "#             'logger should be either a logging.Logger object, '\n",
    "#             f'\"silent\" or None, but got {type(logger)}')\n",
    "\n",
    "\n",
    "# def close_logger(logger):\n",
    "#     handlers = logger.handlers[:]\n",
    "#     for handler in handlers:\n",
    "#         logger.removeHandler(handler)\n",
    "#         handler.close()\n",
    "\n",
    "\n",
    "# def back_to_original(image):\n",
    "#     image = image * torch.tensor([0.229, 0.224, 0.225])[:, None, None]\n",
    "#     image = image + torch.tensor([0.485, 0.456, 0.406])[:, None, None]\n",
    "#     image = image * 255\n",
    "#     image = image.numpy()\n",
    "#     image = image.astype('uint8')\n",
    "#     image = np.transpose(image, [1, 2, 0])\n",
    "#     return image\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "        self.fc.bias.data.fill_(50) # important\n",
    "\n",
    "    def forward(self, x):\n",
    "        ######### YOUR CODE HERE:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.avg_pool(x).squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class WeightResNet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        ######### YOUR CODE HERE:\n",
    "        super().__init__()\n",
    "        self.resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.resnet34.fc = nn.Linear(512, 1)\n",
    "        self.resnet34.fc.bias.data.fill_(50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet34(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trainloader, optimizer, device):\n",
    "    ######### YOUR CODE HERE:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = nnf.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "\n",
    "def validate(model, valloader, device):\n",
    "    ######### YOUR CODE HERE:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nnf.mse_loss(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The model is running on {device}.\")\n",
    "\n",
    "# training parameters\n",
    "epochs = 150\n",
    "lr = 0.001\n",
    "val_interval = 1\n",
    "batch_size = 8\n",
    "decay_factor = 0.1\n",
    "patience = 20\n",
    "print_interval = 5\n",
    "\n",
    "# save best model state dict and metrics\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '5_weight_regression')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best.pth')\n",
    "save_path_metrics = os.path.join(save_dir_state_dict, 'metrics.pkl')\n",
    "\n",
    "# instantiate dataset and dataloader\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "trainset = WeightDataset(train_weights_path, images_dir)\n",
    "valset = WeightDataset(val_weights_path, images_dir)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)    \n",
    "\n",
    "# Initialize model, optimizer and scheduler\n",
    "model = WeightResNet34().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=decay_factor, patience=patience)\n",
    "\n",
    "# train loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train_one_epoch(model, optimizer, trainloader, device)\n",
    "    train_losses.append(train_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if epoch % val_interval == 0:\n",
    "        val_loss = validate(model, valloader, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "        min_val_loss = val_loss\n",
    "        \n",
    "    if epoch % print_interval == 0:\n",
    "        print(f'Epoch {epoch} - train loss: {train_loss:.3f} - val loss: {val_loss:.3f}')\n",
    "    \n",
    "    metrics = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    metrics.to_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.ylim([0,200])\n",
    "\n",
    "####################### plot losses\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['train_loss'], c='blue', label='Training Loss')\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['val_loss'], c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(val_losses)\n",
    "plt.plot(index+1, val_losses[index], 'kx', label='Min Validation Loss')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "# val_weights = pd.read_csv(val_weights_path)\n",
    "images_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "\n",
    "# dataset and trained model\n",
    "valset = WeightDataset(val_weights_path, images_dir)\n",
    "valloader = DataLoader(valset, batch_size=1)\n",
    "model = WeightResNet34()\n",
    "best_ckpt = torch.load('results/best.pth')\n",
    "model.load_state_dict(best_ckpt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(valloader):\n",
    "        image = image.to(device)\n",
    "        pred = model(image)\n",
    "        preds.append(pred.item())\n",
    "        targets.append(target.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(targets, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
