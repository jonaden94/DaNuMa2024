{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import matplotlib.image as mpimg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use a convolutional neural network to predict the weight of pigs using depth images. In principle, this notebook reproduces methodology from the following paper: https://www.sciencedirect.com/science/article/pii/S0168169921000740 \\\n",
    "You will see that applied problems can sometimes be successfully tackled with standard architectures and training procedures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the data. The csv files have as the first column the weight of a pig and as the second column a folder name. \\\n",
    "images_base_dir contains folders with these names where multiple frames of a video recording of the respective pig are located. The videos were recorded with a *depth measuring* camera. \\\n",
    "Explore the csv data:\n",
    "* How many training and validation samples do we have?\n",
    "* What is the mean, standard deviation, maximum and minimum of the weights?\n",
    "* Hint: all of these metrics can be obtained by built in methods of the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular data\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "train_weights = pd.read_csv(train_weights_path)\n",
    "val_weights = pd.read_csv(val_weights_path)\n",
    "\n",
    "# images directory\n",
    "images_base_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "\n",
    "# look at the data frame\n",
    "print(train_weights.head())\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "# explore csv data\n",
    "print('\\n')\n",
    "print(f'number of training examples: {train_weights.shape[0]}')\n",
    "print(f'mean training weight: {train_weights.weight.mean()}')\n",
    "print(f'std of the training weight: {train_weights.weight.std()}')\n",
    "print(f'variance of the training weight: {train_weights.weight.var()}')\n",
    "print(f'min training weight: {train_weights.weight.min()}')\n",
    "print(f'max training weight: {train_weights.weight.max()}')\n",
    "print('\\n')\n",
    "print(f'number of validation examples: {val_weights.shape[0]}')\n",
    "print(f'mean validation weight: {val_weights.weight.mean()}')\n",
    "print(f'std of the validation weight: {val_weights.weight.std()}')\n",
    "print(f'variance of the validation weight: {val_weights.weight.var()}')\n",
    "print(f'min validation weight: {val_weights.weight.min()}')\n",
    "print(f'max validation weight: {val_weights.weight.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the pig images of one example pig. For this, we just obtain the images_folder of, say the fifth row of the train dataframe, and generate a list of all frame names that are located in this folder. We then sort it (you do not have to worry about this) to plot the images in the right temporal order to recreate the video. In the resulting plot you can see the temporal progression. Looking at the images, you can already infer a lot about the data.\n",
    "* How many channels do the images have? (you can also verify this if you want by converting the images to numpy arrays or tensors)\n",
    "* How is depth encoded in these images?\n",
    "\n",
    "\n",
    "######### YOUR ANSWER HERE:\n",
    "* We can see that the colors of the image require multiple channels as they have different shades of e.g. blue and red. Judging from the colors the images are most likely encoded with RGB values.\n",
    "* We can see that the gradient from shallow to deep is represented by a gradient from blue to red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frames of one pig\n",
    "index = 5\n",
    "images_one_pig_dir = os.path.join(images_base_dir, train_weights['images_folder'][index])\n",
    "images_one_pig = os.listdir(images_one_pig_dir)\n",
    "images_one_pig = sorted(images_one_pig, key=lambda x: int(x[:-4].split('_')[-1]))\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 10\n",
    "figsize = (20, 5)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    if i < len(images_one_pig):\n",
    "        img_path = os.path.join(images_one_pig_dir, images_one_pig[i])\n",
    "        img = mpimg.imread(img_path)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off') \n",
    "    else:\n",
    "        axs[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By converting the image to a tensor, we can verify what we already see in the plotted images: They are encoded with three channels (specificlly in the RGB format)\n",
    "img_path = os.path.join(images_one_pig_dir, images_one_pig[0])\n",
    "image = Image.open(img_path)\n",
    "image = transforms.ToTensor()(image)\n",
    "print(image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a Pytorch dataset that loads the images together with the corresponding weight label. This is the first time that the data we have is, so to say, \"distributed\" across different locations (the labels are saved in the csv while the images are somewhere else). However, this is no problem for the Pytorch dataset. You can just pass all the information required to the constructor and then perform all the operations needed to load the images in the getitem function. Do the following:\n",
    "\n",
    "* Pass the weights dataframe (or the path to it) and the images_base_dir to the constructor.\n",
    "* As always, the getitem function gets called with an index as an argument. You can use it to index the dataframe and get the corresponding weight and image folder.\n",
    "* Since there are many images in a single image folder (as we saw above), it might be helpful for training heterogeneity to randomly select one of these images.\n",
    "* Then you can load the image using the ``Image.open()``function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDataset(Dataset):\n",
    "    ######### YOUR CODE HERE:\n",
    "    def __init__(self, weights_df_path, images_base_dir):\n",
    "        self.weights_df = pd.read_csv(weights_df_path)\n",
    "        self.images_base_dir = images_base_dir\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomVerticalFlip(p=0.5),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], # standard normalization for models pretrained\n",
    "                                                    std=[0.229, 0.224, 0.225])\n",
    "                                ])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # select row from dataframe and get data from it\n",
    "        info = self.weights_df.iloc[i, :]\n",
    "        weight = torch.tensor(info.weight).float()\n",
    "        images_folder = info.images_folder\n",
    "\n",
    "        # load one random image corresponding to the weighting of the selected row\n",
    "        images_dir = os.path.join(self.images_base_dir, images_folder)\n",
    "        image_name = random.choice(os.listdir(images_dir))\n",
    "        image_path = os.path.join(images_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # transform and return image\n",
    "        image = self.transform(image)\n",
    "        return image, weight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.weights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to define the model. We can make use of pre-trained models that are publicly available with torchvision and adapt them from our purposes. This way we can make use of the already learned features to have faster convergence and sometimes also better performance, especially when training datasets are small. One popular architecture is resnet. In torchvision, you can get different resnet sizes from torchvision.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT) # load resnet pre-trained on imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model structure using summary! Does the model already have the right output structure? We want to predict the weight. So we want a single output unit.\n",
    "\n",
    "######### YOUR ANSWER HERE \\\n",
    "No, the output of the model has 1000 elements (since imagenet has 1000 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 3, 240, 426))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model does not have the correct output structure, you need to modify it accordingly. Using the following code you can see the name of the parameters or modules of the model. In contrast to the summary function that displays the class names of the modules (e.g. \"Linear\" for nn.Linear), model.named_modules return the actual names of the modules within the model object (e.g. \"fc\" if you set self.fc = nn.Linear(512, 1)). You can use this information to *access/modify/replace* these modules as you would always do with attributes of an object. model.named_parameters only shows those attributes of the model that are parameterized (e.g. a linear layer has parameters while the relu function does not have any parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and try modifying the model so that it can be used for the prediction task at hand. Hint: You will not need to change much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet18 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        ######### YOUR CODE HERE:\n",
    "        self.resnet18.fc = nn.Linear(512, 1)\n",
    "        self.resnet18.fc.bias.data.fill_(55) # initialize bias to the mean of the training pig weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x).squeeze() # squeeze important for the loss function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training functions can be taken basically without modifications from the MLP exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trainloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = nnf.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "\n",
    "def validate(model, valloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nnf.mse_loss(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is also the same in principle. Two new things were added: \n",
    "1) Here we try out a learning rate scheduler, specifically the ReduceLROnPlateau scheduler. It checks whether the validation loss did not increase for a specified number of epochs and, if not, decrease the learning rate by a specified factor.\n",
    "2) In addition to the state_dict, we print and save the metrics (train loss, val loss and also the learning rate) after every epoch to a pickle file (csv would also be possible). This is usually recommended since trainings in practice can take hours or days and you do not want to lose all the information in case something crashes. This training will also take a little longer than the ones from the previous exercises.\n",
    "\n",
    "Start the training procedure now:\n",
    "\n",
    "* Monitor the training loss. Compare it with the variance of the weight in the training dataset. Does the value reduce to a reasonable value after a couple of epochs? \n",
    "* If not, abort the training and think about potential reasons.\n",
    "\n",
    "Hints: \n",
    "* Newly initialized layers will get weights that are randomly distributed around zero. Given this, what is the expected prediction of the network when it has not yet been trained? \n",
    "* What could we change to influence the initial prediction in a favorable way given that the linear layer is defined as $output = w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + bias$\n",
    "\n",
    "######### YOUR ANSWER HERE \\\n",
    "Since the weights of the final linear layer are randomly initialized around zero, the expected prediction at the beginning of training is also zero. This is very far off from the mean weight which is roughly 55. The initially expected mean squared error therefore is very high (roughly 50 * 50 = 2500). This can lead to unstable and large gradients. Intuitively, one could say that the network is so far off that it does not really know how to adjust its parameters. The network might even not converge at all in some cases! We can help out by setting the bias to the mean of the dataset at the beginning of training (see model definition of the solution).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The model is running on {device}.\")\n",
    "\n",
    "# training parameters\n",
    "epochs = 120\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "print_interval = 1 ### print loss every print_interval epochs\n",
    "decay_factor = 0.1 ### factor to reduce learning rate\n",
    "patience = 20 ### number of epochs to wait before reducing learning rate\n",
    "\n",
    "# save best model state dict and metrics\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '5_weight_regression')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best.pth')\n",
    "save_path_metrics = os.path.join(save_dir_state_dict, 'metrics.pkl')\n",
    "\n",
    "# instantiate dataset and dataloader\n",
    "train_weights_path = os.path.join(raw_data_dir, '5_weight_regression/train.csv')\n",
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "images_base_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "trainset = WeightDataset(train_weights_path, images_base_dir)\n",
    "valset = WeightDataset(val_weights_path, images_base_dir)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False)    \n",
    "\n",
    "# Initialize model, optimizer and scheduler\n",
    "model = WeightModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=decay_factor, patience=patience)\n",
    "\n",
    "# train loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "min_val_loss = float('inf')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss = train_one_epoch(model, trainloader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_loss = validate(model, valloader, device)\n",
    "    scheduler.step(val_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "        min_val_loss = val_loss\n",
    "        \n",
    "    if epoch % print_interval == 0:\n",
    "        print(f'Epoch {epoch} - train loss: {train_loss:.3f} - val loss: {val_loss:.3f} - lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "    \n",
    "    metrics = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    metrics.to_pickle(save_path_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the results. For the loss curves, we can use the same functions as before. Since the loss might be very high at the start of training, it might be useful to clip the ylim of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)\n",
    "\n",
    "####################### plot losses\n",
    "# plt.ylim([0,50])\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['train_loss'], c='blue', label='Training Loss')\n",
    "plt.plot(np.linspace(1, epochs, epochs), results['val_loss'], c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(results['val_loss'])\n",
    "plt.plot(index+1, results['val_loss'][index], 'kx', label='Min Validation Loss', markersize=12)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot figure 8 from the paper. \n",
    "* Get all predictions and ground truths on the validation dataset using the best model.\n",
    "* Note that there will be some variability in the predictions in case you randomly sample an image for each pig.\n",
    "* Plot the results in the same way as in the paper.\n",
    "* also calculate the mean squared error given all the predictions and ground truths. It should be similar to the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_weights_path = os.path.join(raw_data_dir, '5_weight_regression/val.csv')\n",
    "images_base_dir = os.path.join(raw_data_dir, '5_weight_regression/images')\n",
    "\n",
    "# dataset and trained model\n",
    "valset = WeightDataset(val_weights_path, images_base_dir)\n",
    "valloader = DataLoader(valset, batch_size=1)\n",
    "model = WeightModel()\n",
    "best_ckpt = torch.load(save_path_state_dict)\n",
    "model.load_state_dict(best_ckpt)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for image, target in tqdm(valloader):\n",
    "        image = image.to(device)\n",
    "        pred = model(image)\n",
    "        all_preds.append(pred.item())\n",
    "        all_targets.append(target.item())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "plt.scatter(all_targets, all_preds)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Predicted')\n",
    "\n",
    "# Plot reference line for perfect prediction\n",
    "min_val = min(min(all_targets), min(all_preds))\n",
    "max_val = max(max(all_targets), max(all_preds))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean squared error (should give comparable results to the validation loss)\n",
    "mse = np.mean((all_targets - all_preds) ** 2)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus (advanced): defining a resnet by yourself\n",
    "\n",
    "Instead of modifying the torchvision implementation of a resnet, you can also define your own resnet. Take a look at the original paper if you want: https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
    "\n",
    "1) Start by implementing the `BasicBlock` (basic residual block) described in the paper\n",
    "* It consists of two convolutional layers and a residual connection.\n",
    "* Of course batch norm and relu need to be added in the right places\n",
    "* A residual connection using a pure identity function is in general not possible since the number of channels usually changes with the convolutions. A projection of each pixel in the input feature map to the respective dimension is what comes closest to a pure identity. This can be implemented as a convolution with kernel size 1x1.\n",
    "* Also implement a downsampling using MaxPool2d conditional on a boolean downsample argument. Downsample by a factor of 2.\n",
    "* If no downsampling is performed, the output should have the same width height dimensions, so add appropriate padding.\n",
    "\n",
    "2) Then implement a ``Block`` module that combines two `BasicBlock` modules.\n",
    "* The second basic block should downsample.\n",
    "\n",
    "3) Finally, implement the ``ResNet``\n",
    "* It should (I) apply multiple ``Block`` modules, (II) pool the final resulting feature map and (III) apply a fully connected layer.\n",
    "\n",
    "It should be noted that the resulting model will most likely perform worse than the torchvision model (I obtained a validation mse of roughly 10). This is probably due to a better implementation of the torchvision model and the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic building block for a ResNet model.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels to the module.\n",
    "        out_channels (int): Number of output channels after the second conv layer.\n",
    "        downsample (bool, optional): Whether to apply downsampling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.identity = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        if downsample:\n",
    "            self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "        x = nnf.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += identity\n",
    "        x = nnf.relu(x)\n",
    "        \n",
    "        if self.downsample:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom block used in the ResNet model.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels to the module.\n",
    "        out_channels (int): Number of output channels after the second BasicBlock.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        self.block1 = BasicBlock(in_channels, out_channels)\n",
    "        self.block2 = BasicBlock(out_channels, out_channels, downsample=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom implementation of the ResNet architecture.\n",
    "\n",
    "    Args:\n",
    "        block_channels (list of int): A list of integers where the ith and (i+1)th entry represent the in_channels and out_channels of the ith Block layer.\n",
    "        output_size (int): The size of the final output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, block_channels, output_size):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        \n",
    "        # blocks\n",
    "        blocks = []\n",
    "        for i in range(len(block_channels) - 1):\n",
    "            blocks.append(Block(block_channels[i], block_channels[i+1]))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        # output layer\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(block_channels[-1], output_size)\n",
    "        self.fc.bias.data.fill_(55) # initialize bias to the mean of the training pig weights\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1) # equivalent to squeezing but maybe nice to see some alternative code :)\n",
    "        x = self.fc(x)\n",
    "        return x         \n",
    "    \n",
    "    \n",
    "model = ResNet([3, 32, 64, 128, 256], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
