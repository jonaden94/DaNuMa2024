{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import os\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will enhance the MLP architecture from the last exercise with a well-known regularization technique, namely \"dropout\". Furthermore, you will implement a convolutional neural network and demonstrate its superiority over the MLP when it comes to image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR10 dataset is a well-known dataset for image classification. The torchvision library directly implements functionality for creating a torch dataset for this and many other standard datasets. For faster training, we only consider every 5th image by subsetting the original dataset. The inputs to a neural network are usually normalized. We use the training dataset to calculate the mean RGB value across all images and pixel values as well as their standard deviation. Then we define a transformation that normalizes input tensors with these values: $x_{norm} = (x - mean_x) / std_x$. \\\n",
    "We use the torchvision.transforms object which is a popular transformation tool for images that implements many transformations. What other transformations / augmentations could be added? Feel free to read the torchvision.transforms documentation and add further components to the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate normalization values\n",
    "data_root = os.path.join(raw_data_dir, '4_convnet')\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_root, train=True, transform=torchvision.transforms.ToTensor())\n",
    "every_fifth_idx = list(range(0, len(trainset), 5))\n",
    "trainset = torch.utils.data.Subset(trainset, every_fifth_idx)\n",
    "\n",
    "num_samples = trainset.dataset.data.shape[0]\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=num_samples, \n",
    "                                            num_workers=2)\n",
    "imgs, _ = next(iter(trainloader))\n",
    "# the argument \"dim\" defines over which dimesions the mean and standard deviation of a tensor is calculated \n",
    "# In this case: dim 0=batch and dim (2, 3) = (height, width) --> we get the mean RGB value since the channel dimension is dim 1\n",
    "dataset_mean = torch.mean(imgs, dim=(0, 2, 3))\n",
    "dataset_std = torch.std(imgs, dim=(0, 2, 3))\n",
    "\n",
    "normalized_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(dataset_mean, dataset_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the normalize transform we just defined we can now instantiate the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and validation set and dataloaders\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_root, train=True, transform=normalized_transform)\n",
    "every_fifth_idx = list(range(0, len(trainset), 5))\n",
    "trainset = torch.utils.data.Subset(trainset, every_fifth_idx)\n",
    "valset = torchvision.datasets.CIFAR10(root=data_root, train=False, transform=normalized_transform)\n",
    "every_fifth_idx = list(range(0, len(valset), 5))\n",
    "valset = torch.utils.data.Subset(valset, every_fifth_idx)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(valset, batch_size=1000, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same normalize transform for both the training and validation set. Why do we not use the validation set for the calculation of the normalization values?\n",
    "\n",
    "######### YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize 10 of the images to get an idea about what kind of dataset we are dealing with. When running the code as it is, you will get a warning that the pixels have unexpected values and the colors of the images look a little bit off. \n",
    "* Why is that the case? Transform the images to look normal again! Hint: Broadcasting might come in handy for the back-transformation.\n",
    "* What is the size of the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize images\n",
    "classes = trainset.dataset.classes\n",
    "print(classes)\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 5))\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for ax in axs:\n",
    "    # select random image from the batch\n",
    "    batch = next(iter(trainloader))\n",
    "    images, labels = batch\n",
    "    random_idx = torch.randint(0, len(labels), (1,))\n",
    "    image = images[random_idx].squeeze()\n",
    "    label = labels[random_idx]\n",
    "\n",
    "    ######### YOUR CODE HERE:\n",
    "    \n",
    "    # plot the image together with the class name\n",
    "    class_name = classes[label]\n",
    "    ax.set_title(class_name)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will investigate how well MLPs are able to deal with the class prediction task. Since images are high-dimensional inputs, we will naturally need bigger models compared to the MLP exercise where we had a single input value. However, using bigger models also increases the risk of overfitting because there are more neurons available with which the model can learn the inputs by heart! There are various possibilities to prevent overfitting such as *data augmentation*, *l2 regularization*, *using a validation set* or *dropout*. In this exercise, we will specifically explore the use of dropout. Your task is to modify the MLP code from the last exercise to fulfil the following:\n",
    "\n",
    "* Linear layers take one-dimensional data as input. Images are multi-dimensional. Modify the network in a way that images can be processed by these linear layers. Hint: you might have encountered helpful functionality in the pytorch introduction :)\n",
    "* read sections 1, 2, 3 and 5 of this short medium article about dropout: https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9\n",
    "* Modify the network to include dropout after each linear layer except the last one. The last layer makes the prediction so we do not want too much uncertainty there.\n",
    "* When instantiating the model, you should be able to define the dropout ratio. Add it as an argument to the constructor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code!\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            modules.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "            if not i == len(layer_sizes)-2:\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the summary function from the torchinfo package (already imported) to visualize the model structure. It takes as first argument the model (need to instantiate first) and as the second argument the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we can use the same train and validation functions as in the last exercise (see cell below). However, you have to modify them a little bit for the task at hand:\n",
    "\n",
    "* Images are already multi-dimensional. So adding an empty dimension is not necessary anymore.\n",
    "* In addition to the loss, the functions should also return the accuracy (percentage of correctly predicted classes). Hint: You can use the argmax function to obtain the prediction and compare it with the ground truth labels. The average accuracy across the whole dataset can be calculate in the same way as the average loss.\n",
    "* We do not use the mse loss anymore but rather need the cross entropy loss (nnf.cross_entropy) to compare the predicted probabilities with the ground truth classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code!\n",
    "def train_one_epoch(model, trainloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in trainloader:\n",
    "        x_batch = x_batch[:, None].float().to(device)\n",
    "        y_batch = y_batch[:, None].float().to(device)\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = nnf.mse_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n",
    "def validate(model, valloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valloader:\n",
    "            x_batch = x_batch[:, None].float().to(device)\n",
    "            y_batch = y_batch[:, None].float().to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = nnf.mse_loss(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the train_one_epoch and validate functions are defined, you should be able to run the below code block to train the model. Try out different dropout ratios. How does it affect the training and validation accuracy?\n",
    "\n",
    "######### YOUR ANSWER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"The model is running on {device}.\")\n",
    "\n",
    "# training parameters\n",
    "epochs = 25\n",
    "lr = 0.001\n",
    "val_interval = 1\n",
    "\n",
    "# save best model state dict\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '4_convnet')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best_mlp.pth')\n",
    "\n",
    "# instantiate model and optimizer\n",
    "layer_sizes = [3*32*32, 384, 384, 384, 10]\n",
    "dropout_ratio = 0.3\n",
    "model = MLP(layer_sizes, dropout_ratio).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_accuracy = train_one_epoch(model, trainloader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    if epoch % val_interval == 0:\n",
    "        val_loss, val_accuracy = validate(model, valloader, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### plot losses\n",
    "plt.plot(np.linspace(1, epochs, epochs), train_losses, c='blue', label='Training Loss')\n",
    "plt.plot(np.linspace(1, epochs, epochs), val_losses, c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(val_losses)\n",
    "plt.plot(index+1, val_losses[index], 'kx', label='Min Validation Loss')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### plot accuracies\n",
    "plt.plot(np.linspace(1, epochs, epochs), train_accuracies, c='blue', label='Training accuracy')\n",
    "plt.plot(np.linspace(1, epochs, epochs), val_accuracies, c='red', label='Validation accuracy')\n",
    "\n",
    "# Mark the maximum validation accuracy\n",
    "index = np.argmax(val_accuracies)\n",
    "plt.plot(index+1, val_accuracies[index], 'kx', label='max Validation accuracy')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print maximum accuracy\n",
    "print(f'maximum validation accuracy: {np.max(val_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MLPs for image classification tasks certainly works to some extent. However, we already discussed that it is certainly not optimal since it does not make use of inherent image characteristics, such as recurring features at different positions. To learn such reatures, convolutional neural networks are the model of choice:\n",
    "\n",
    "* Implement a simple convolutional neural network with 5 convolutional layers (kernel_size=3, stride=1, padding='valid', channel sizes [3, 32, 32, 64, 64, 64])\n",
    "* Each convolutional layer should be followed by a batch norm and relu layer\n",
    "* The feature map after these 5 layers should be pooled (e.g. with nn.AdaptiveAvgPool2d) and then mapped onto 10 classes with a final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    ######### YOUR CODE HERE:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, you can use the summary function from the torchinfo package (already imported) to visualize the model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use exactly the same training loop as before and only need to change the model! This is the beauty of code modularity. Since the model has exactly the same input and output shape we can just plug it in and leave the rest unchanged. \n",
    "* Train the model with the standard training parameters provided in the above training loop.\n",
    "* Visualize the loss and accuracy also using the code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curves\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is a rough measure of model performance. To get a more detailed picture of the model's strengths and weaknesses, we can look at the confusion matrix. The confusion matrix requires as input a list or numpy array of all predicted and ground truth labels.\n",
    "* Obtain all predictions and ground truth labels using your model with the best validation loss. Hint: The code is very similar to the one in the validate function.\n",
    "* Look at the resulting confusion matrix. Which classes are confused most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valset and classes\n",
    "valset = torchvision.datasets.CIFAR10(root=data_root, train=False, transform=normalized_transform)\n",
    "classes = valset.classes\n",
    "every_fifth_idx = list(range(0, len(valset), 5))\n",
    "valset = torch.utils.data.Subset(valset, every_fifth_idx)\n",
    "valloader = DataLoader(valset, batch_size=1000, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "# load the best model\n",
    "# get all predictions and ground truth labels\n",
    "all_labels = None\n",
    "all_preds = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Display the confusion matrix with class names\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "If you still have time, you can try to tune some of the hyperparameters, modernize the architecture a little bit (e.g. strided convolutions), add data augmentations etc. and try to nudge the validation performance even higher. Everything over 60% accuracy is certainly good :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further reads:\n",
    "\n",
    "These reads require some understanding of mathematical notation. \\\n",
    "original dropout paper: https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf \\\n",
    "There are many articles on regularization in neural networks in general. Here is one of them: https://www.pinecone.io/learn/regularization-in-neural-networks/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
