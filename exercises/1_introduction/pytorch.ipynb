{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch introduction\n",
    "\n",
    "Credits are given to the official Introduction to Pytorch from which some parts of this notebook were taken: https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this notebook in google colab, set colab to True else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting data directory\n",
    "import os\n",
    "if colab:\n",
    "    # define download url\n",
    "    base = 'https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/9AIY3V'\n",
    "    folder = '3ZFUWQ'\n",
    "    download_url = os.path.join(base, folder)\n",
    "\n",
    "    # define save paths\n",
    "    save_name_zip = '1_introduction.zip'\n",
    "    raw_data_folder = 'data/raw_data'\n",
    "    save_data_folder = 'data/output_data'\n",
    "\n",
    "    # make data directories\n",
    "    !mkdir -p $raw_data_folder\n",
    "    !mkdir -p $save_data_folder\n",
    "\n",
    "    # download and unzip data\n",
    "    !wget -O $save_name_zip $download_url\n",
    "    !unzip $save_name_zip -d $raw_data_folder\n",
    "    !rm -rf $save_name_zip\n",
    "    \n",
    "    home_dir = '/content'\n",
    "    raw_data_dir = os.path.join(home_dir, 'data/raw_data')\n",
    "    output_data_dir = os.path.join(home_dir, 'data/output_data')\n",
    "else:\n",
    "    exercise_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    danuma_dir = os.path.dirname(os.path.dirname(exercise_dir))\n",
    "    raw_data_dir = os.path.join(danuma_dir, 'data/raw_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you are looking for a Pytorch function that does something specific or you want to know what a certain function or class does in Pytorch, \\\n",
    "you can always google the command and take a look at the official Pytorch documentation. \\\n",
    "For example, this is the documentation for the torch.Tensor object: https://pytorch.org/docs/stable/tensors.html \\\n",
    "Let's start this intro by importing torch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see a few basic tensor manipulations. First, just a few of the ways to create tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros(5, 3)\n",
    "print(z)\n",
    "print(z.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create a 5x3 matrix filled with zeros, and query its datatype to find out that the zeros are 32-bit floating point numbers, which is the default PyTorch. \\\n",
    "What if you wanted integers instead? You can always override the default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.ones((5, 3), dtype=torch.int16)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that when we do change the default, the tensor helpfully reports this when printed. \\\n",
    "To create a tensor with numbers randomly drawn from a uniform distribution between 0 and 1, you can use the torch.randn function. \\\n",
    "For reproducibility, it is often useful to set a seed so that the same random numbers are drawn when the function is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2) # returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
    "print('A random tensor:')\n",
    "print(r1)\n",
    "\n",
    "r2 = torch.rand(2, 2)\n",
    "print('\\nA different random tensor:')\n",
    "print(r2) # new values\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "r3 = torch.rand(2, 2)\n",
    "print('\\nShould match r1:')\n",
    "print(r3) # repeats values of r1 because of re-seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors perform arithmetic operations intuitively. Tensors of similar shapes may be added, multiplied, etc. Operations with scalars are distributed over the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
    "print(twos)\n",
    "\n",
    "threes = ones + twos       # addition allowed because shapes are similar\n",
    "print(threes)              # tensors are added element-wise\n",
    "print(threes.shape)        # this has the same dimensions as input tensors\n",
    "\n",
    "r1 = torch.rand(2, 3)\n",
    "r2 = torch.rand(3, 2)\n",
    "# uncomment this line to get a runtime error because shapes do not fit together\n",
    "# r3 = r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a small sample of the mathematical operations available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.sin(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also many other useful commands. To give a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of uniformly distributed random numbers, there are also other random number generators available, for example:\n",
    "ints = torch.randint(0, 10, (10,))\n",
    "print('Random integers:')\n",
    "print(ints)\n",
    "gaussians = torch.randn(10)\n",
    "print('Random numbers from standard normal distribution:')\n",
    "print(gaussians)\n",
    "\n",
    "# Instead of getting the maximum or minimum value, it is often useful to get the index of the maximum or minimum value:\n",
    "r = torch.rand(10)\n",
    "print('\\nA random vector, r:')\n",
    "print(r)\n",
    "print('Indice of the maximum value in r:')\n",
    "print(torch.argmax(r))\n",
    "print('Indice of the minimum value in r:')\n",
    "print(torch.argmin(r))\n",
    "\n",
    "# creates regularly spaced numbers between two values\n",
    "linspace = torch.linspace(1, 10, 10)\n",
    "print('\\nRegularly spaced numbers between 1 and 10:')\n",
    "print(linspace)\n",
    "\n",
    "# numpy arrays can be converted to tensors and vice versa\n",
    "import numpy as np\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.from_numpy(a) # shares memory with numpy array, as opposed to using 'torch.tensor(a)'\n",
    "print('\\nNumpy array:')\n",
    "print(a)\n",
    "print('Tensor from numpy array:')\n",
    "print(t)\n",
    "print('Numpy array from tensor:')\n",
    "print(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors in Pytorch are objects. As you know, objects have attributes and methods. These are often useful to obtain characteristics of tensor. \\\n",
    "In fact, the functionality of many functions is also directly available as a tensor method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.rand(10)\n",
    "print('\\nA random vector, r:')\n",
    "print(r)\n",
    "print('\\nIndice of the maximum value in r:')\n",
    "print(torch.argmax(r))\n",
    "print('\\nYou can also obtain the indice of the maximum value via a method instead of an external function:')\n",
    "print(r.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as numpy, Pytorch can easily represent multi-dimensional objects such as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # PIL is a Python Imaging Library\n",
    "import torchvision.transforms as transforms # torchvision offers useful tools for image processing\n",
    "from IPython.display import display\n",
    "\n",
    "# Open the image file\n",
    "image_path = os.path.join(raw_data_dir, '1_introduction/test1.jpg')\n",
    "image = Image.open(image_path)\n",
    "display(image)\n",
    "\n",
    "# Convert to tensor\n",
    "transform = transforms.ToTensor()\n",
    "image = transform(image)\n",
    "print(f'Tensor shape: {image.shape}')\n",
    "print(f'Tensor type: {image.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Pytorch represents images as three-dimensional tensors. \\\n",
    "The first dimension represents the red, green and blue values for each pixel. \\\n",
    "The second and third dimension represent the height and width of the image. \\\n",
    "Indexing a tensor works just as in numpy, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get pixel values at the center of the image:')\n",
    "print(image[:, 180, 320])\n",
    "\n",
    "print('\\nGet only the red-channel of the image')\n",
    "print(image[0, :, :].shape)\n",
    "print(image[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.ToTensor() automatically normalized pixel values to the interval (0, 1). \\\n",
    "The actual intensity for pixels is between 0 and 255. \\\n",
    "To get the original unnormalized values, you can simply multiply them with 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unnormalized pixel values:')\n",
    "print(image[:, 180, 320] * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how the individual color channels contribute to the image by setting the other values to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_copy = image.clone()\n",
    "image_copy[[0, 2], ...] = 0\n",
    "\n",
    "transform = transforms.ToPILImage()\n",
    "image_copy = transform(image_copy)\n",
    "display(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add dimensions to an existing tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tensor = torch.randn(3, 224, 224)\n",
    "\n",
    "example_tensor_with_added_dimension = example_tensor.unsqueeze(0)\n",
    "print('\\nAdded dimension at first position:')\n",
    "print(example_tensor_with_added_dimension.shape)\n",
    "\n",
    "example_tensor_with_added_dimension = example_tensor.unsqueeze(1)\n",
    "print('\\nAdded dimension at second position:')\n",
    "print(example_tensor_with_added_dimension.shape)\n",
    "\n",
    "example_tensor_with_added_dimension = example_tensor[None, None, :, :, :, None]\n",
    "print('\\nIf you want to add dimensions at the beginning or at the end, you can also just index the whole tensor and add None:')\n",
    "print(example_tensor_with_added_dimension.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redundant dimensions that only have a size of one can also be removed again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing redundant dimensions that only have a size of one can be done with squeeze:')\n",
    "print(example_tensor_with_added_dimension.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple tensors are often stacked together in a batch so that they can be processed together. \\\n",
    "This can be done by adding a new dimension and then stacking the tensor along this dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 4 pseudo-images using list comprehension\n",
    "images = [torch.randn(3, 224, 224) for _ in range(4)]\n",
    "# add dimension at first position for each of these images, again using list comprehension\n",
    "images = [image.unsqueeze(0) for image in images]\n",
    "# stack these images along the first dimension to obtain a batch of images\n",
    "images = torch.cat(images, dim=0)\n",
    "\n",
    "print('The first dimension of the images tensor represents the batch:')\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing operations on high-dimensional tensors, it is often helpful to make use of so-called \"broadcasting\". \\\n",
    "In broadcasting, one tensor that is involved in the operation of interest is implicitly enlarged by duplicating values along other dimensions. \\\n",
    "Then the operation of interest is performed on this enlarged version. Let's have a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a pseudo-image first\n",
    "image = torch.randn(3, 224, 224)\n",
    "print(f'The shape of the image tensor: {image.shape}')\n",
    "# we want to multiply the red channel by 0, the green channel by 0, and the blue channel by 1\n",
    "multiplier = torch.tensor([0, 0, 1])\n",
    "# We add dimensions to the multiplier tensor to make it compatible with the image tensor\n",
    "multiplier = multiplier[:, None, None]\n",
    "print(f'The shape of the multiplier tensor: {multiplier.shape}')\n",
    "# If we now compute the product of the image tensor and the multiplier tensor, \n",
    "# each red, green and blue value of the image will be multiplied by the corresponding value in the multiplier tensor\n",
    "product = image * multiplier\n",
    "# All red and green values will be zeros\n",
    "print('\\nThe red and green values are zero:')\n",
    "print(product[[0, 1], :, :])\n",
    "# All blue values will be the same as in the original image\n",
    "print('\\nThe blue values are the same as in the original image:')\n",
    "print(product[2, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch with gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if a gpu is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cuda is available, you can put tensors on the gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(3, 224, 224)\n",
    "image = image.cuda() \n",
    "# image = image.to('cuda') # does the same job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing computations, make sure that all tensors involved are on the same device (i.e. cuda or cpu):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor1 = torch.randn(64, 10).cuda()\n",
    "test_tensor2 = torch.randn(64, 10).cuda()\n",
    "elementwise_product = test_tensor1 * test_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to a cpu that performs arithmetic operations one by one, a gpu performs operations simultaneously. \\\n",
    "That means that all 640 operations required to perform the elementwise product are performed at the same time in the previous example. \\\n",
    "This massively speeds up training and inference time for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two tensors are not on the same device and you try to perform operations, this will result in an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor1 = torch.randn(64, 10).cuda()\n",
    "test_tensor2 = torch.randn(64, 10)\n",
    "elementwise_product = test_tensor1 * test_tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch modules (model components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how tensors are being processed by pytorch modules. \\\n",
    "We start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # the parent object for PyTorch models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model component is a linear layer that simply computes a weighted sum of the input. \\\n",
    "For demonstration purposes, the layer is initialized without a bias and with a constant value for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(in_features=10, out_features=1, bias=False)\n",
    "torch.nn.init.constant_(linear_layer.weight, 1.0)\n",
    "print(linear_layer.weight)\n",
    "print(f'bias: {linear_layer.bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically initialized a linear model where all parameters w1, ..., w10 have a value of 1 and there is no intercept (called bias in the context of machine learning). \\\n",
    "Since the parameter values are all 1, this linear layer just computes the sum of the input. \\\n",
    "We could also initialize multiple linear models when setting out_features > 1. Then multiple weighted sums of the input would be computed. \\\n",
    "We can apply the module to an input tensor by calling it like a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(10)\n",
    "result = linear_layer(input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the model on a tensor that also consists only of ones, we of course get 10 as a result. \\\n",
    "The attributes requires_grad of the weights and grad_fn of the result indicate that these tensors will be taken into account when a gradient is computed. \\\n",
    "We will take about this tomorrow, so feel free to ignore this right now.\\\n",
    "\\\n",
    "We can also apply the linear layer on a batch of inputs. This will independently apply the module on each individual input in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets fill the first input of the batch with ones, the second with twos, the third with threes, and so on...\n",
    "inputs = torch.ones(64, 10)\n",
    "multiplier = torch.arange(1, 65, 1)\n",
    "multiplier = multiplier[:, None]\n",
    "inputs = inputs * multiplier\n",
    "# we can now pass the inputs to the linear layer\n",
    "output = linear_layer(inputs)\n",
    "print(f'Shape of the output: {output.shape}')\n",
    "print(f'As expected, the first output is 10, the second is 20, the third is 30, and so on: \\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a tensor of size 100 with random numbers, obtain a new tensor that contains the 10 largest elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of random numbers\n",
    "torch.manual_seed(1729)\n",
    "r = torch.rand(100)\n",
    "print(r)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given a tensor of size 64x100, obtain for each row the index of the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of random numbers\n",
    "torch.manual_seed(1729)\n",
    "r = torch.rand(64, 100)\n",
    "print(r)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replicate the result from the broadcasting example from the Pytorch dimensions section without broadcasting. \\\n",
    "Hint: You can use the torch.tile function to obtain a tensor with the same size of the image and perform elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a pseudo-image first\n",
    "image = torch.randn(3, 224, 224)\n",
    "print(f'The shape of the image tensor: {image.shape}')\n",
    "# we want to multiply the red channel by 0, the green channel by 0, and the blue channel by 1\n",
    "multiplier = torch.tensor([0, 0, 1])\n",
    "# We add dimensions to the multiplier tensor to make it compatible with the image tensor\n",
    "multiplier = multiplier[:, None, None]\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Given a tensor of size 10, obtain a new tensor of size 100 where each element in the original tensor is repeated 10 times. \\\n",
    "This is different from repeating the full tensor 10 times which could be done with torch.tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.arange(10)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What does the torch.flatten function do? Apply the function to a batch of pseudo images. \\\n",
    "Only flatten the images and not the batch dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.rand(64, 3, 224, 224)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define and apply a linear layer with 5 in_features and 2 out_features. Apply the layer and inspect the output shape and values. \\\n",
    "Obtain the same output shape by manually defining a (random) weights matrix and multiplying the inputs with it using matrix multiplication. \\\n",
    "In case you are unfamiliar with the way matrix-vector or matrix-matrix multiplication works, have a look at this visualization: http://matrixmultiplication.xyz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(8, 5)\n",
    "print(inputs)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What does the nn.ReLu module do? Apply the module on a random tensor and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.randn(100)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. You already applied a linear layer with a specific number of input and output features to a batch of inputs in exercise 5. \\\n",
    "Now your task is to alternately apply mutliple linear layers and relu functions successively to a batch of inputs. \\\n",
    "The three linear layers should have the following input/output feature sizes: 5/10, 10/20 and 20/1. \\\n",
    "Each linear layer except for the last one should be followed by a relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(8, 5)\n",
    "print(inputs.shape)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Bonus: We did not yet talk about why it makes sense to stack multiple linears and relu functions behind each other. \\\n",
    "So do not worry about this in too much detail now. However, to provide some food for thought: \\\n",
    "Do you have an idea why it does NOT make sense to simply stack multiple linear layers behind each other? \\\n",
    "How is this counteracted by adding relu functions? \\\n",
    "Justify your answer in words (or a sketch the idea of a small mathematical proof)\n",
    "\n",
    "Hint: Consider the representational capacity of multiple linear layers stacked behind each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR ANSWER HERE (code is not necessarily required): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further learning sources\n",
    "\n",
    "If you do not yet feel quite comfortable with tensors (i.e. matrices and vectors) and their operations, I recommend you to have a look at the three blue one brown linear Algebra series (and his channel in general). He offers amazing teaching videos. However, they are probably too extensive to watch and fully understand during the course of this summer school: https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
