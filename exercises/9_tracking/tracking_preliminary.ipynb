{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import random\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# detection\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "############################# TO DELETE\n",
    "home_dir = '/usr/users/henrich1'\n",
    "#############################\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize tracking and save both individual frames and a video\n",
    "def visualize_mot(images_dir, tracks_file, output_dir, video_output_path, ids_to_visualize=None, bbox_linewidth=2, id_size=1, fps=10):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    # Load tracking data from file\n",
    "    tracks = {}\n",
    "    with open(tracks_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = [int(i) if i.isdigit() else float(i) for i in line.split(',')]\n",
    "            frame_id, obj_id, x_min, y_min, x_max, y_max = data # assuming the format is [x_min, y_min, x_max, y_max]\n",
    "            if frame_id not in tracks:\n",
    "                tracks[frame_id] = []\n",
    "            tracks[frame_id].append((obj_id, x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    # Get the list of image files (frames)\n",
    "    frame_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Initialize output directory for frames and video\n",
    "    frames_output_dir = os.path.join(output_dir, 'frames')\n",
    "    os.makedirs(frames_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the video writer\n",
    "    first_frame = cv2.imread(os.path.join(images_dir, frame_files[0]))\n",
    "    height, width, _ = first_frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Initialize a dictionary to store bbox colors for each obj_id\n",
    "    colors = {}\n",
    "    \n",
    "    # Iterate over frames and draw bounding boxes\n",
    "    for frame_idx, frame_file in enumerate(tqdm(frame_files)):\n",
    "        frame_id = frame_idx + 1  # Assuming frame ID is based on the order of files\n",
    "        frame_path = os.path.join(images_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        # If the frame contains tracking data, draw the bounding boxes\n",
    "        if frame_id in tracks:\n",
    "            for obj_id, x_min, y_min, x_max, y_max in tracks[frame_id]:\n",
    "                if ids_to_visualize is not None and obj_id not in ids_to_visualize:\n",
    "                    continue\n",
    "                \n",
    "                # Assign a random color to the object if it's not already in the dictionary\n",
    "                if obj_id not in colors:\n",
    "                    colors[obj_id] = tuple(map(int, np.random.choice(range(256), size=3)))\n",
    "                \n",
    "                color = colors[obj_id]\n",
    "                \n",
    "                # Cast x_min, y_min, x_max, y_max to integers\n",
    "                x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, bbox_linewidth)\n",
    "                \n",
    "                # Draw object ID label\n",
    "                cv2.putText(frame, str(obj_id), (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, id_size, color, 2)\n",
    "        \n",
    "        # Save the output frame with bounding boxes\n",
    "        output_frame_path = os.path.join(frames_output_dir, frame_file)\n",
    "        cv2.imwrite(output_frame_path, frame)\n",
    "        \n",
    "        # Write the frame to the video\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    # Release the video writer\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/henrich1/repos/PigDetect/mmdetection/projects/CO-DETR/codetr/transformer.py:1325: UserWarning: If you want to reduce GPU memory usage,                               please install fairscale by executing the                               following command: pip install fairscale.\n",
      "  warnings.warn('If you want to reduce GPU memory usage, \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_conv.bias - torch.Size([256]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_cls.weight - torch.Size([9, 256, 1, 1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_cls.bias - torch.Size([9]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_reg.weight - torch.Size([36, 256, 1, 1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_reg.bias - torch.Size([36]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_cls.bias - torch.Size([2]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.gn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.gn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.gn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.gn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_cls.weight - torch.Size([1, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_cls.bias - torch.Size([1]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_reg.weight - torch.Size([4, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_centerness.weight - torch.Size([1, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_centerness.bias - torch.Size([1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.0.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.1.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.2.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.3.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.4.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/22 17:35:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.5.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "Loads checkpoint by local backend from path: /usr/users/henrich1/repos/DaNuMa2024/data/raw_data/7_segmentation/pretrained/codino_swin.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/henrich1/repos/PigDetect/mmdetection/mmdet/models/dense_heads/anchor_head.py:108: UserWarning: DeprecationWarning: `num_anchors` is deprecated, for consistency or also use `num_base_priors` instead\n",
      "  warnings.warn('DeprecationWarning: `num_anchors` is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "# 1. initialize the model\n",
    "config_path = os.path.join(home_dir, 'repos/PigDetect/configs/co-detr/co_dino_swin.py')\n",
    "checkpoint_codino_path = os.path.join(raw_data_dir, '7_segmentation/pretrained/codino_swin.pth')\n",
    "register_all_modules(init_default_scope=False)\n",
    "model = init_detector(config_path, checkpoint_codino_path, device='cuda:0') # cuda:0 for gpu\n",
    "\n",
    "# 2. run model inference on image\n",
    "image_path = os.path.join(raw_data_dir, '7_segmentation/images/danuma_1578.jpg')\n",
    "image = mmcv.imread(image_path, channel_order='rgb')\n",
    "result = inference_detector(model, image)\n",
    "\n",
    "scores = result.pred_instances.scores.cpu().numpy()\n",
    "bboxes = result.pred_instances.bboxes.cpu().numpy()\n",
    "bboxes = bboxes[scores > 0.5] # filter boxes by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. calculate all bboxes in frame 1\n",
    "2. then for following frame also calculate the bboxes\n",
    "3. calculate iou between all boxes in frame 1 and 2 to get iou matrix\n",
    "4. perform hungarian matching to associate detections into tracks\n",
    "5. do this for all frames of the video and save the tracks in an easily human readable format (the same that is required by visualize_tracking_mot)\n",
    "6. visualize the tracks using visualize_tracking_mot i provided. you need to change visualize_tracking_mot so that it works directly with frames and not with mp4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(raw_data_dir, '9_tracking/video1/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate IoU between two bounding boxes\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two bounding boxes.\n",
    "    Args:\n",
    "        bbox1, bbox2: Bounding boxes [x_min, y_min, x_max, y_max]\n",
    "    Returns:\n",
    "        iou: Intersection over Union\n",
    "    \"\"\"\n",
    "    x_min1, y_min1, x_max1, y_max1 = bbox1\n",
    "    x_min2, y_min2, x_max2, y_max2 = bbox2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x_min_inter = max(x_min1, x_min2)\n",
    "    y_min_inter = max(y_min1, y_min2)\n",
    "    x_max_inter = min(x_max1, x_max2)\n",
    "    y_max_inter = min(y_max1, y_max2)\n",
    "    \n",
    "    if x_max_inter < x_min_inter or y_max_inter < y_min_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    # Intersection area\n",
    "    intersection_area = (x_max_inter - x_min_inter) * (y_max_inter - y_min_inter)\n",
    "    \n",
    "    # Areas of the bboxes\n",
    "    bbox1_area = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
    "    bbox2_area = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
    "    \n",
    "    # Union area\n",
    "    union_area = bbox1_area + bbox2_area - intersection_area\n",
    "    \n",
    "    # IoU\n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bboxes(bboxes_frame1, bboxes_frame2, iou_threshold):\n",
    "    \"\"\"\n",
    "    Match bounding boxes between two frames using IoU and Hungarian Algorithm.\n",
    "    Args:\n",
    "        bboxes_frame1: Bounding boxes in frame 1 (list of [x_min, y_min, x_max, y_max])\n",
    "        bboxes_frame2: Bounding boxes in frame 2 (list of [x_min, y_min, x_max, y_max])\n",
    "    Returns:\n",
    "        matches: List of tuples where each tuple is (index_in_frame1, index_in_frame2)\n",
    "    \"\"\"\n",
    "    iou_matrix = np.zeros((len(bboxes_frame1), len(bboxes_frame2)))\n",
    "\n",
    "    for i, bbox1 in enumerate(bboxes_frame1):\n",
    "        for j, bbox2 in enumerate(bboxes_frame2):\n",
    "            iou_matrix[i, j] = calculate_iou(bbox1, bbox2)\n",
    "    \n",
    "    # Perform Hungarian matching (maximize IoU by minimizing negative IoU)\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "    \n",
    "    # Filter matches based on IoU threshold (e.g., ignore low IoU matches)\n",
    "    matches = []\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        if iou_matrix[r, c] > iou_threshold:  # Threshold for valid IoU match\n",
    "            matches.append((r, c))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking function for all frames\n",
    "def track_objects_in_video(images_dir, model, output_track_path, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Perform object detection on each frame and track objects using IoU-based matching.\n",
    "    Args:\n",
    "        images_dir: Directory containing images (frames)\n",
    "        model: Initialized object detection model\n",
    "        output_track_path: Path to save the tracking result in MOT format\n",
    "        iou_threshold: IoU threshold for matching\n",
    "    \"\"\"\n",
    "    frame_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    tracks = []\n",
    "    next_track_id = 0\n",
    "    active_tracks = {}\n",
    "    \n",
    "    for frame_idx, frame_file in tqdm(enumerate(frame_files)):\n",
    "        # Load frame\n",
    "        frame_path = os.path.join(images_dir, frame_file)\n",
    "        frame = mmcv.imread(frame_path, channel_order='rgb')\n",
    "        \n",
    "        # Run detection\n",
    "        result = inference_detector(model, frame)\n",
    "        bboxes = result.pred_instances.bboxes.cpu().numpy()\n",
    "        scores = result.pred_instances.scores.cpu().numpy()\n",
    "        bboxes = bboxes[scores > 0.5]\n",
    "        \n",
    "        if frame_idx == 0:\n",
    "            # Initialize new tracks in the first frame\n",
    "            for bbox in bboxes:\n",
    "                tracks.append([frame_idx + 1, next_track_id, *bbox])\n",
    "                active_tracks[next_track_id] = bbox\n",
    "                next_track_id += 1\n",
    "        else:\n",
    "            # Match current frame bboxes with previous frame tracks\n",
    "            previous_bboxes = active_tracks.values()\n",
    "            matches = match_bboxes(previous_bboxes, bboxes, iou_threshold)\n",
    "            \n",
    "            # Update existing tracks with matched bboxes\n",
    "            matched_tracks = set()\n",
    "            for match in matches:\n",
    "                track_idx, bbox_idx = match\n",
    "                track_id = list(active_tracks.keys())[track_idx]\n",
    "                tracks.append([frame_idx + 1, track_id, *bboxes[bbox_idx]])\n",
    "                active_tracks[track_id] = bboxes[bbox_idx]\n",
    "                matched_tracks.add(track_id)\n",
    "\n",
    "            # Remove inactive tracks\n",
    "            inactive_tracks = set(active_tracks.keys()) - matched_tracks\n",
    "            for track_id in inactive_tracks:\n",
    "                del active_tracks[track_id]\n",
    "                \n",
    "            # Start new tracks for unmatched bboxes\n",
    "            unmatched_bboxes = set(range(len(bboxes))) - {m[1] for m in matches}\n",
    "            for bbox_idx in unmatched_bboxes:\n",
    "                tracks.append([frame_idx + 1, next_track_id, *bboxes[bbox_idx]])\n",
    "                active_tracks[next_track_id] = bboxes[bbox_idx]\n",
    "                next_track_id += 1\n",
    "            \n",
    "    # Save tracks to file\n",
    "    with open(output_track_path, 'w') as f:\n",
    "        for track in tracks:\n",
    "            f.write(','.join(map(str, track)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [00:40,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir_tracking = os.path.join(output_data_dir, '9_tracking/video1')\n",
    "os.makedirs(output_dir_tracking, exist_ok=True)\n",
    "tracking_results_path = os.path.join(output_dir_tracking, 'tracking_results.txt')\n",
    "track_objects_in_video(images_dir, model, tracking_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [00:05<00:00, 29.44it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "visualize_mot(\n",
    "    images_dir=images_dir, \n",
    "    tracks_file=tracking_results_path, \n",
    "    output_dir=output_dir_tracking,\n",
    "    video_output_path=os.path.join(output_dir_tracking, 'tracking_results.mp4')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
