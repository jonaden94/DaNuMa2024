{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import random\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# detection\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement a simple algorithm for multiple-object tracking to track pigs in videos. We will employ the so-called \"tracking-by-detection\" paradigm. In this paradigm, an object detector is used to generate bounding boxes for every frame. Then, some algorithm to match these bounding boxes with each other is used to obtain tracks. In this exercise, the matching rule you will implement is very simple and could be improved in many ways. However, it is still the basis of many modern mutli-object-tracking frameworks (see further reads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in exercise 7, we will use a pre-trained object detector for pigs. The code is already provided below. If you want to know details about the pretrained pig detection framework, take a look at the repository at https://github.com/jonaden94/PigDetect/ and the corresponding demo notebook at https://github.com/jonaden94/PigDetect/blob/main/tools/inference/inference_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_conv.bias - torch.Size([256]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_cls.weight - torch.Size([9, 256, 1, 1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_cls.bias - torch.Size([9]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_reg.weight - torch.Size([36, 256, 1, 1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "rpn_reg.bias - torch.Size([36]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uni08/hpc/henrich1/u12112/repos/PigDetect/mmdetection/projects/CO-DETR/codetr/transformer.py:1325: UserWarning: If you want to reduce GPU memory usage,                               please install fairscale by executing the                               following command: pip install fairscale.\n",
      "  warnings.warn('If you want to reduce GPU memory usage, \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_cls.weight - torch.Size([2, 1024]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_cls.bias - torch.Size([2]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_reg.weight - torch.Size([4, 1024]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.0.bias - torch.Size([1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "bbox_head.shared_fcs.1.bias - torch.Size([1024]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.gn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_convs.0.gn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.gn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "reg_convs.0.gn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_cls.weight - torch.Size([1, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_cls.bias - torch.Size([1]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_reg.weight - torch.Size([4, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_centerness.weight - torch.Size([1, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "atss_centerness.bias - torch.Size([1]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.0.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.1.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.2.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.3.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.4.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "09/26 02:43:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "scales.5.scale - torch.Size([]): \n",
      "The value is the same before and after calling `init_weights` of CoATSSHead  \n",
      " \n",
      "Loads checkpoint by local backend from path: /user/henrich1/u12112/repos/DaNuMa2024/data/raw_data/7_instance_segmentation/pretrained/codino_swin.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uni08/hpc/henrich1/u12112/repos/PigDetect/mmdetection/mmdet/models/dense_heads/anchor_head.py:108: UserWarning: DeprecationWarning: `num_anchors` is deprecated, for consistency or also use `num_base_priors` instead\n",
      "  warnings.warn('DeprecationWarning: `num_anchors` is deprecated, '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoint_codino_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7_instance_segmentation/pretrained/codino_swin.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m register_all_modules(init_default_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minit_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_codino_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# cuda:0 for gpu\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2. run model inference on image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7_instance_segmentation/images/danuma_1578.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/repos/PigDetect/mmdetection/mmdet/apis/inference.py:114\u001b[0m, in \u001b[0;36minit_detector\u001b[0;34m(config, checkpoint, palette, device, cfg_options)\u001b[0m\n\u001b[1;32m    111\u001b[0m             model\u001b[38;5;241m.\u001b[39mdataset_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpalette\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    113\u001b[0m model\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m config  \u001b[38;5;66;03m# save the config in the model for convenience\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:208\u001b[0m, in \u001b[0;36mBaseModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(torch\u001b[38;5;241m.\u001b[39mdevice(device))\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/torch/nn/modules/module.py:844\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/uni08/hpc/henrich1/u12112/conda/envs/danuma/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# 1. initialize the model\n",
    "config_path = os.path.join(home_dir, 'repos/PigDetect/configs/co-detr/co_dino_swin.py')\n",
    "checkpoint_codino_path = os.path.join(raw_data_dir, '7_instance_segmentation/pretrained/codino_swin.pth')\n",
    "register_all_modules(init_default_scope=False)\n",
    "model = init_detector(config_path, checkpoint_codino_path, device='cuda:0') # cuda:0 for gpu\n",
    "\n",
    "# 2. run model inference on image\n",
    "image_path = os.path.join(raw_data_dir, '7_instance_segmentation/images/danuma_1578.jpg')\n",
    "image = mmcv.imread(image_path, channel_order='rgb')\n",
    "result = inference_detector(model, image)\n",
    "\n",
    "# # this is how you get scores and bboxes\n",
    "# scores = result.pred_instances.scores.cpu().numpy()\n",
    "# bboxes = result.pred_instances.bboxes.cpu().numpy()\n",
    "# bboxes = bboxes[scores > 0.5] # filter boxes by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of tracking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``track_objects_in_video`` function given below takes as input a directory where all frames of a video are saved (\"images_dir\") and a pretrained object detection model (\"model\"). The pretrained object detection model is used to generate detections for every frame. To match the bounding boxes of two consecutive frames with each other, the function ``match_bboxes`` function is called. It calculates for every pair of bounding boxes from the two consecutive frames how much they overlap. For example, if there are M detections in the first frame and N detections in the second frame, M $\\cdot$ N overlap metrics are computed. The overlap metric that we use here is the so-called **IoU**. Your task in this exercise is to implement the function ``calculate_iou``:\n",
    "* Take a look at the iou_example.svg image in the exercise folder to understand what the IoU is.\n",
    "* Hint: Keep in mind that the minimum y-value of a box is at the top (see iou_example.svg)! This is because for images the y-coordinate zero is at the top. This is just a convention but it is also in line with our intuition about matrices (images are just matrices, right?) where we start counting the rows from the top. The rows represent the y-axis.\n",
    "\n",
    "To calculate the IoU, perform the following steps:\n",
    "1. get the minimum x/y and maximum x/y values for both boxes\n",
    "2. from these, infer a condition when the bounding boxes do not overlap at all. In this case, immediately return 0.\n",
    "3. If the bounding boxes overlap, calculate the intersection and union and return it.\n",
    "\n",
    "The other parts of the code are already complete and the tracking algorithm should work if you correctly implemented ``calculate_iou``! :) . However, you might want to take a look at the matching algorithm in the ``match_bboxes`` function to understand how the matching is done. Basically, we calculate the IoU for every pair of bounding boxes in two consecutive frames. So, if there are for example N bounding boxes in frame 1 and M bounding boxes in frame 2, we calculate an IoU matrix of size M $\\cdot$ N. Then we choose the matching that maximizes the total IoU. For this, we use the ``linear_sum_assignment`` function from scipy. The underlying algorithm is relatively complex. Google \"Hungarian algorithm\" if you want to know more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two bounding boxes.\n",
    "    Args:\n",
    "        bbox1, bbox2: Bounding boxes [x_min, y_min, x_max, y_max]\n",
    "    Returns:\n",
    "        iou: Intersection over Union\n",
    "    \"\"\"\n",
    "    ################ YOUR CODE HERE:\n",
    "    x_min1, y_min1, x_max1, y_max1 = bbox1\n",
    "    x_min2, y_min2, x_max2, y_max2 = bbox2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x_min_inter = max(x_min1, x_min2)\n",
    "    y_min_inter = max(y_min1, y_min2)\n",
    "    x_max_inter = min(x_max1, x_max2)\n",
    "    y_max_inter = min(y_max1, y_max2)\n",
    "    \n",
    "    if x_max_inter < x_min_inter or y_max_inter < y_min_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    # Intersection area\n",
    "    intersection_area = (x_max_inter - x_min_inter) * (y_max_inter - y_min_inter)\n",
    "    \n",
    "    # Areas of the bboxes\n",
    "    bbox1_area = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
    "    bbox2_area = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
    "    \n",
    "    # Union area\n",
    "    union_area = bbox1_area + bbox2_area - intersection_area\n",
    "    \n",
    "    # IoU\n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bboxes(bboxes_frame1, bboxes_frame2, iou_threshold):\n",
    "    \"\"\"\n",
    "    Match bounding boxes between two frames using IoU and Hungarian Algorithm.\n",
    "    Args:\n",
    "        bboxes_frame1: Bounding boxes in frame 1 (list of [x_min, y_min, x_max, y_max])\n",
    "        bboxes_frame2: Bounding boxes in frame 2 (list of [x_min, y_min, x_max, y_max])\n",
    "    Returns:\n",
    "        matches: List of tuples where each tuple is (index_in_frame1, index_in_frame2)\n",
    "    \"\"\"\n",
    "    iou_matrix = np.zeros((len(bboxes_frame1), len(bboxes_frame2)))\n",
    "\n",
    "    for i, bbox1 in enumerate(bboxes_frame1):\n",
    "        for j, bbox2 in enumerate(bboxes_frame2):\n",
    "            iou_matrix[i, j] = calculate_iou(bbox1, bbox2)\n",
    "    \n",
    "    # Perform Hungarian matching (maximize IoU by minimizing negative IoU)\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "    \n",
    "    # Filter matches based on IoU threshold (e.g., ignore low IoU matches)\n",
    "    matches = []\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        if iou_matrix[r, c] > iou_threshold:  # Threshold for valid IoU match\n",
    "            matches.append((r, c))\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking function for all frames\n",
    "def track_objects_in_video(images_dir, model, output_track_path, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Perform object detection on each frame and track objects using IoU-based matching.\n",
    "    Args:\n",
    "        images_dir: Directory containing images (frames)\n",
    "        model: Initialized object detection model\n",
    "        output_track_path: Path to save the tracking result in MOT format\n",
    "        iou_threshold: IoU threshold for matching\n",
    "    \"\"\"\n",
    "    frame_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    tracks = []\n",
    "    next_track_id = 0\n",
    "    active_tracks = {}\n",
    "    \n",
    "    for frame_idx, frame_file in tqdm(enumerate(frame_files)):\n",
    "        # Load frame\n",
    "        frame_path = os.path.join(images_dir, frame_file)\n",
    "        frame = mmcv.imread(frame_path, channel_order='rgb')\n",
    "        \n",
    "        # Run detection\n",
    "        result = inference_detector(model, frame)\n",
    "        bboxes = result.pred_instances.bboxes.cpu().numpy()\n",
    "        scores = result.pred_instances.scores.cpu().numpy()\n",
    "        bboxes = bboxes[scores > 0.5]\n",
    "        \n",
    "        if frame_idx == 0:\n",
    "            # Initialize new tracks in the first frame\n",
    "            for bbox in bboxes:\n",
    "                tracks.append([frame_idx + 1, next_track_id, *bbox])\n",
    "                active_tracks[next_track_id] = bbox\n",
    "                next_track_id += 1\n",
    "        else:\n",
    "            # Match current frame bboxes with previous frame tracks\n",
    "            previous_bboxes = active_tracks.values()\n",
    "            matches = match_bboxes(previous_bboxes, bboxes, iou_threshold)\n",
    "            \n",
    "            # Update existing tracks with matched bboxes\n",
    "            matched_tracks = set()\n",
    "            for match in matches:\n",
    "                track_idx, bbox_idx = match\n",
    "                track_id = list(active_tracks.keys())[track_idx]\n",
    "                tracks.append([frame_idx + 1, track_id, *bboxes[bbox_idx]])\n",
    "                active_tracks[track_id] = bboxes[bbox_idx]\n",
    "                matched_tracks.add(track_id)\n",
    "\n",
    "            # Remove inactive tracks\n",
    "            inactive_tracks = set(active_tracks.keys()) - matched_tracks\n",
    "            for track_id in inactive_tracks:\n",
    "                del active_tracks[track_id]\n",
    "                \n",
    "            # Start new tracks for unmatched bboxes\n",
    "            unmatched_bboxes = set(range(len(bboxes))) - {m[1] for m in matches}\n",
    "            for bbox_idx in unmatched_bboxes:\n",
    "                tracks.append([frame_idx + 1, next_track_id, *bboxes[bbox_idx]])\n",
    "                active_tracks[next_track_id] = bboxes[bbox_idx]\n",
    "                next_track_id += 1\n",
    "            \n",
    "    # Save tracks to file\n",
    "    with open(output_track_path, 'w') as f:\n",
    "        for track in tracks:\n",
    "            f.write(','.join(map(str, track)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply apply the tracking functionality from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to frames of the video\n",
    "images_dir = os.path.join(raw_data_dir, '9_tracking/video1/images')\n",
    "\n",
    "# path to save tracking results\n",
    "output_dir_tracking = os.path.join(output_data_dir, '9_tracking/video1')\n",
    "os.makedirs(output_dir_tracking, exist_ok=True)\n",
    "tracking_results_path = os.path.join(output_dir_tracking, 'tracking_results.txt')\n",
    "\n",
    "# call the tracking function\n",
    "track_objects_in_video(images_dir, model, tracking_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tracking results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to visualize the tracks. It just plots the bounding boxes on each image and creates a video. The color of a bounding box represents the predicted id! Inspect the visualization. \n",
    "* Where do mistakes happen and why? \n",
    "* How could we improve the tracker?\n",
    "\n",
    "################### YOUR ANSWER HERE: \\\n",
    "We can see that, if the detector has problems, tracks immediately terminate since a match is not found. \\\n",
    "This could be alleviated by e.g. adding a patience where tracks are not lost immediately but rather keps im memory for a couple of frames. \\\n",
    "However, this does not alleviate the problem if objects that were not detected for a couple of frames moved or if the detector has severe problems. \\\n",
    "More recent frameworks (see furhter reads) try to mitigate the reliance on external object detectors by actually LEARNING the whole tracking process without external detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize tracking and save both individual frames and a video\n",
    "def visualize_mot(images_dir, tracks_file, output_dir, video_output_path, ids_to_visualize=None, bbox_linewidth=2, id_size=1, fps=10):\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    # Load tracking data from file\n",
    "    tracks = {}\n",
    "    with open(tracks_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = [int(i) if i.isdigit() else float(i) for i in line.split(',')]\n",
    "            frame_id, obj_id, x_min, y_min, x_max, y_max = data # assuming the format is [x_min, y_min, x_max, y_max]\n",
    "            if frame_id not in tracks:\n",
    "                tracks[frame_id] = []\n",
    "            tracks[frame_id].append((obj_id, x_min, y_min, x_max, y_max))\n",
    "    \n",
    "    # Get the list of image files (frames)\n",
    "    frame_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    # Initialize output directory for frames and video\n",
    "    frames_output_dir = os.path.join(output_dir, 'frames')\n",
    "    os.makedirs(frames_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the video writer\n",
    "    first_frame = cv2.imread(os.path.join(images_dir, frame_files[0]))\n",
    "    height, width, _ = first_frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Initialize a dictionary to store bbox colors for each obj_id\n",
    "    colors = {}\n",
    "    \n",
    "    # Iterate over frames and draw bounding boxes\n",
    "    for frame_idx, frame_file in enumerate(tqdm(frame_files)):\n",
    "        frame_id = frame_idx + 1  # Assuming frame ID is based on the order of files\n",
    "        frame_path = os.path.join(images_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        \n",
    "        # If the frame contains tracking data, draw the bounding boxes\n",
    "        if frame_id in tracks:\n",
    "            for obj_id, x_min, y_min, x_max, y_max in tracks[frame_id]:\n",
    "                if ids_to_visualize is not None and obj_id not in ids_to_visualize:\n",
    "                    continue\n",
    "                \n",
    "                # Assign a random color to the object if it's not already in the dictionary\n",
    "                if obj_id not in colors:\n",
    "                    colors[obj_id] = tuple(map(int, np.random.choice(range(256), size=3)))\n",
    "                \n",
    "                color = colors[obj_id]\n",
    "                \n",
    "                # Cast x_min, y_min, x_max, y_max to integers\n",
    "                x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, bbox_linewidth)\n",
    "                \n",
    "                # Draw object ID label\n",
    "                cv2.putText(frame, str(obj_id), (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, id_size, color, 2)\n",
    "        \n",
    "        # Save the output frame with bounding boxes\n",
    "        output_frame_path = os.path.join(frames_output_dir, frame_file)\n",
    "        cv2.imwrite(output_frame_path, frame)\n",
    "        \n",
    "        # Write the frame to the video\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    # Release the video writer\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mot(\n",
    "    images_dir=images_dir, \n",
    "    tracks_file=tracking_results_path, \n",
    "    output_dir=output_dir_tracking,\n",
    "    video_output_path=os.path.join(output_dir_tracking, 'tracking_results.mp4')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further reads\n",
    "* ground-breaking paper that first introduced the basic tracking idea employed in this exercise: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533003\n",
    "* example of further developments of this method that builds more sophisticated matching rule: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296962\n",
    "* more recent frameworks try learning tracking (not only pretrained object detector with some association rule). Examples are: https://openaccess.thecvf.com/content/CVPR2022/papers/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.pdf \\\n",
    "https://arxiv.org/pdf/2105.03247 \\\n",
    "https://arxiv.org/pdf/2211.09791\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python danuma",
   "language": "python",
   "name": "danuma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
