{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf68330",
   "metadata": {},
   "source": [
    "# Instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139ed2b",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import random\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import torch\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# detection\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "# segmentation\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exercise_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "danuma_dir = os.path.dirname(os.path.dirname(exercise_dir))\n",
    "raw_data_dir = os.path.join(danuma_dir, 'data/raw_data')\n",
    "output_data_dir = os.path.join(danuma_dir, 'data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac5126",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b131a2",
   "metadata": {},
   "source": [
    "Implementing neural networks on your own is an important skill for becoming an applied machine learning researcher. It makes you to think about how neural networks work, which strengthens your understanding of the subject. However, it is **equally important** to be able to quickly modify and combine existing code bases and packages for your purposes. This is the goal of this notebook. You will (1) use existing functionality for pig detection to obtain bounding boxes on unlabeled images from a pig barn, and (2) use these bounding boxes as input to SAM (acronym for \"Segment Anything Model\") to obtain instance segmentations.\n",
    "\n",
    "It should be noted that it is not important to fully understand the methods you use. In this exercise, the focus lies on understanding how to use and adapt the functionality provided by other repositories to solve tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abec695",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01498df7",
   "metadata": {},
   "source": [
    "To be used later for plotting bounding boxes, points and instance segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c3571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax, random_color=False):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "\n",
    "    if random_color:\n",
    "        edge_color = (random.random(), random.random(), random.random())\n",
    "    else:\n",
    "        edge_color = 'green'\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=edge_color, facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "      \n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4796f16",
   "metadata": {},
   "source": [
    "### 1. Pig detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0299e135",
   "metadata": {},
   "source": [
    "For pig detection, we will rely on a pig detection repository that is part of ongoing research in the DaNuMa project: https://github.com/jonaden94/PigDetect. \\\n",
    "When you open the repository in a browser, you will see the ReadMe right under the repository's directory structure. This ReadMe usually contains all the information on how to use the repository (e.g. setup, download of relevant files, reference to demo notebooks). In practice, you would first have to clone the repository yourself, install the packages of interest (setup) and download the pretrained model weights. For the sake of this exercise, the repository is already cloned (``repos/PigDetect``) and all necessary installations and downloads have been performed. So don't worry about it at this point! The object detection model we will use is called \"codino\". A link to the paper is in the \"Further reads\" section in case you are interested (not relevant for the exercise!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71285db0",
   "metadata": {},
   "source": [
    "The demo notebook in the PigDetect repository (https://github.com/jonaden94/PigDetect/blob/main/tools/inference/inference_demo.ipynb) provides code to use pretrained models for pig detection. Inspect the notebook and copy/modify the relevant code from it:\n",
    "1. Initialize the codino model. You will need to adjust ``config_path`` and ``checkpoint_path``. The config is located at ``repos/PigDetect/configs/co-detr/co_dino_swin.py`` and the pretrained model weights are under ``repos/DaNuMa2024/data/raw_data/7_instance_segmentation/pretrained/codino_swin.pth``\n",
    "2. Run model inference on one of the example images under ``DaNuMa2024/data/raw_data/7_instance_segmentation/images``\n",
    "3. Plot the bounding boxes to verify that the model works (code already provided below). Do you obtain the correct bounding boxes? Hint: You might need to filter the bounding boxes based on their score!\n",
    "\n",
    "You will get some warnings even if the code is correct. You can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0549c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 1. initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66e937d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 2. run model inference on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b295b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. plot bounding boxes\n",
    "bboxes = [] # DELETE THIS LINE ONCE YOU OBTAINED THE REAL BBOXES\n",
    "image_path = os.path.join(raw_data_dir, '7_instance_segmentation/images/danuma_1578.jpg') # path to the image you obtained the bboxes for\n",
    "image = mmcv.imread(image_path, channel_order='rgb')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for box in bboxes:\n",
    "    show_box(box, plt.gca(), random_color=True)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {},
   "source": [
    "### 2. Instance mask from single-point prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eefd68",
   "metadata": {},
   "source": [
    "For instance segmentation, we will rely on the repository of the SAM model introduced by Facebook AI Research: https://github.com/facebookresearch/segment-anything. \\\n",
    "The model generates segmentation masks for objects on images without the need to retrain the model explicitly for the task at hand! (hence the name \"Segment Anything\"). If you want to know more about this ground-breaking model, a link to the paper is given in the \"Further reads\" section (not relevant for this exercise). \\\n",
    "Once again, all necessary packages have already been installed and pretrained models are already downloaded. In this part of the notebook, we will use the model with a **point prompt**. That means: We will provide the model with a point on the image and, if the model lives up to its name, it hopefully generates a segmentation mask that represents the object that was marked with the point. For this, we do not need the bounding boxes yet.\n",
    "\n",
    "There is also a demo notebook for SAM (https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb) that provides all functionality you need. If you want to try out the demo notebook before you work on this exercise, you can do so by clicking on the \"open in colab\" button. A quick guide on how to use google colab is provided in the pad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71775621",
   "metadata": {},
   "source": [
    "For this exercise, you do **NOT** need to install/import any packages. All relevant functions are already imported and can be directly used in this notebook, so you can ignore any installation/import statements in the demo notebook. Inspect the notebook and copy/modify the relevant code from it:\n",
    "1. Load the SAM model and predictor. The pretrained model checkpoint for SAM is located at ``repos/DaNuMa2024/data/raw_data/7_instance_segmentation/pretrained/sam_vit_h_4b8939.pth``. The model type is ``vit_h`` and the model should be put on the GPU (cuda).\n",
    "2. Set the image of the predictor to the image from the pig barn.\n",
    "3. Define the point to provide as input to the model (code is already provided).\n",
    "4. Input the point to the predictor to obtain a segmentation mask. As recommended in the demo notebook, set multimask_output=True and then select the mask with the highest score.\n",
    "5. Plot the mask (code is already provided).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same image as for detection\n",
    "image_path = os.path.join(raw_data_dir, '7_instance_segmentation/images/danuma_1578.jpg') # path to the image you obtained the bboxes for\n",
    "image = mmcv.imread(image_path, channel_order='rgb')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8683ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 1. load the SAM model and predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d95d48dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 2. set image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. select and visualize input point based on which to segment\n",
    "input_point = np.array([[700, 350]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 4. input point to predictor to obtain segmentation masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c227a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 5.plot image with mask\n",
    "mask = np.zeros_like(image[..., 0]).astype('bool') # DELETE THIS LINE ONCE YOU OBTAINED THE REAL MASKS\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_mask(mask, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddbca3",
   "metadata": {},
   "source": [
    "### 3. Instance mask from multi-box prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce839a84",
   "metadata": {},
   "source": [
    "SAM is a flexible model. It works with different kinds of prompts. We will now use the bounding boxes as prompts that we obtained in the first part of the exercise. The demo notebook of SAM provides functionality to input all of these bounding boxes at once (batched prompt inputs) to obtain segmentation masks for all pigs. Inspect the notebook and copy/modify the relevant code to do this:\n",
    "\n",
    "1. use the image and bounding boxes that you obtained in the first part of this exercise. You have to convert the bounding boxes to a torch.tensor.\n",
    "2. perform batched inference to obtain instance segmentation masks. What is the shape of the mask and how is it related to the shape of the image? What values does the mask contain?\n",
    "3. plot image with bounding boxes and masks (code is already provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cc6a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 1. image and bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117521a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# 2. get segmentation based on bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. plot image with bounding boxes and segmentation masks\n",
    "bboxes = [] # DELETE THIS LINE ONCE YOU OBTAINED THE REAL BBOXES\n",
    "masks = [] # DELETE THIS LINE ONCE YOU OBTAINED THE REAL MASKS\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "for box in bboxes:\n",
    "    show_box(box.cpu().numpy(), plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e5bbe",
   "metadata": {},
   "source": [
    "### Further reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61efb3",
   "metadata": {},
   "source": [
    "The object detection model used in this notebook: https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf \\\n",
    "Starting with such a complicated object detection model right away might not be the best way to dive into the topic. The following are more simple approaches that have been shaping the field of object detection for years:\n",
    "* https://arxiv.org/pdf/1504.08083 (Fast-RCNN)\n",
    "* https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf (YOLO)\n",
    "* https://people.ee.duke.edu/~lcarin/Christy10.9.2020.pdf (DeTr)\n",
    "\n",
    "Segment Anything Model: https://arxiv.org/pdf/2304.02643"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
