{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language inference with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "from IPython.display import display, HTML\n",
    "# import spacy\n",
    "import os\n",
    "from captum.attr import IntegratedGradients\n",
    "import pickle\n",
    "import json\n",
    "import ssl\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "raw_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/raw_data')\n",
    "output_data_dir = os.path.join(home_dir, 'repos/DaNuMa2024/data/output_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will tackle a task called 'natural language inference'. In this task, there are two sentences and it should be classified which relation *sentence 1* has to *sentence 2*. The three classes are **contradiction**, **neutral** and **entailment**. Let's look at an example for each class:\n",
    "\n",
    "**Contradiction**\n",
    "- *Sentence 1*: The sky is clear and blue.\n",
    "- *Sentence 2*: It is raining heavily.\n",
    "- *Explanation*: These two sentences directly contradict each other. If the sky is clear and blue, it cannot be raining heavily at the same time.\n",
    "\n",
    "**Neutral**\n",
    "- *Sentence 1*: A woman is walking in the park.\n",
    "- *Sentence 2*: She is listening to music.\n",
    "- *Explanation*: These two sentences are neutral because knowing that a woman is walking in the park does not tell us whether she is listening to music. Both can be true, but one doesn’t necessarily imply or contradict the other.\n",
    "\n",
    "**Entailment**\n",
    "- *Sentence 1*: The child is playing with a soccer ball.\n",
    "- *Sentence 2*: A child is playing.\n",
    "- *Explanation*: If the first sentence is true, it logically entails that the second sentence must also be true. If a child is playing with a soccer ball, it’s clear that the child is playing.\n",
    "\n",
    "you will build, train and evaluate a transformer model for this basic natural language classification task. Pytorch already has the basic building blocks of transformers implemented. So you do not need to implement the attention mechanism from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the data. Originally, it is stored as dictionaries in the json format. However, we want to have a more human-readable format so that we extract the relevant information and store it in a pandas dataframe. This is already implemented and performed in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert label to numeric form\n",
    "def label_to_numeric(label):\n",
    "    assert label in ['contradiction', 'neutral', 'entailment']\n",
    "    if label == 'contradiction':\n",
    "        return 0\n",
    "    elif label == 'neutral':\n",
    "        return 1\n",
    "    elif label == 'entailment':\n",
    "        return 2\n",
    "\n",
    "def get_df_from_jsonl(path):\n",
    "    with open(path) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    data_rows = []\n",
    "    for item in tqdm(data):\n",
    "        gold_label = item['gold_label']\n",
    "        sentence1 = item['sentence1']\n",
    "        sentence2 = item['sentence2']\n",
    "        concatenated_sentences = f\"{sentence1} <sep> {sentence2}\"\n",
    "        \n",
    "        # for some sentence pairs, a gold label does not exist. We will not include these in the dataset\n",
    "        if not gold_label in ['contradiction', 'neutral', 'entailment']:\n",
    "            continue\n",
    "        label_numeric = label_to_numeric(gold_label)\n",
    "        \n",
    "        data_rows.append({\n",
    "            'label_text': gold_label,\n",
    "            'label_numeric': label_numeric,\n",
    "            'sentence1': sentence1,\n",
    "            'sentence2': sentence2,\n",
    "            'concatenated': concatenated_sentences\n",
    "        })\n",
    "    return pd.DataFrame(data_rows)\n",
    "    \n",
    "    \n",
    "train_path = os.path.join(raw_data_dir, '8_natural_language_inference/snli_1.0_train.jsonl')\n",
    "val_path = os.path.join(raw_data_dir, '8_natural_language_inference/snli_1.0_dev.jsonl')\n",
    "test_path = os.path.join(raw_data_dir, '8_natural_language_inference/snli_1.0_test.jsonl')\n",
    "\n",
    "train_df = get_df_from_jsonl(train_path)\n",
    "val_df = get_df_from_jsonl(val_path)\n",
    "test_df = get_df_from_jsonl(test_path)\n",
    "\n",
    "# look at the first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a new column in the dataframe ('concatenated') that combines both sentences and separates them with a separator token ('\\<sep\\>'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Example of concatenated sentences: {train_df.loc[0, 'concatenated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concatenated sentences will be used as input to our model. This way, the model has access to both sentences and can reason about the relation between them. However, the model cannot take raw strings as input. Therefore, we first have to convert these strings into numbers. A common approach to convert natural language into a sequence of numbers consists of the following two steps:\n",
    "1. **Tokenize the sentence**. Tokenization describes the process of breaking up natural language into individual building blocks (called 'tokens'). In many cases, these tokens are just words, but there are also exceptions. For example, some tokenizers break up verbs in *gerund* like 'going' into two tokens 'go' and 'ing'. Punctuation marks are also separate tokens. The result of tokenizing a sentence is simply a list of tokens that form the sentence. There are various python libraries that offer readily usable tokenizers.\n",
    "2. **Build vocabulary from tokens to map them onto integer numbers**: Once we have tokens for all of our data, we can use it to build a vocabulary, i.e. the set of all unique tokens in our data. We can then identify each token by a number and use this correspondence to map tokens to numbers. For example, the first entry in our vocabulary might be 'the'. Then, the 'the' token will be mapped onto the number '1'. This way, we will obtain sequences of numbers that represent our natural language input.\n",
    "\n",
    "Since models usually take equally sized examples as input for batching (remember last exercises where this has always been the cases), we will have to ensure that all number sequences have the same length. This is done by selecting a long-enough sequence length (e.g. by looking at the longest input) and then padding sequences that are shorter with zeros until they all have the same length. The process of building the vocabulary and mapping tokens onto numbers takes relatively long. This is why you should not run the two code cells below. The result of running it is already provided and can be directly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at longest sentence\n",
    "# max_len = train_df['concatenated'].apply(lambda x: len(x.split())).max()\n",
    "# print(max_len) \n",
    "# # longest concanaated sentence has 113 words\n",
    "# # --> 150 tokens should be enough to cover (almost) all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')  # Load a small English language model for tokenization\n",
    "\n",
    "# # Tokenization and Vocabulary Building\n",
    "# def tokenize(text):\n",
    "#     return [token.text.lower() for token in nlp(text)]\n",
    "\n",
    "# # Build vocabulary from training data\n",
    "# def build_vocab(texts, start_index=1):\n",
    "#     vocab = {}\n",
    "#     index = start_index\n",
    "#     for text in tqdm(texts):\n",
    "#         tokens = tokenize(text)\n",
    "#         for token in tokens:\n",
    "#             if token not in vocab:\n",
    "#                 vocab[token] = index\n",
    "#                 index += 1\n",
    "#     return vocab\n",
    "\n",
    "# # Build vocabulary\n",
    "# vocab = build_vocab(train_df['concatenated'])\n",
    "# print(f'Vocabulary size: {len(vocab)}')\n",
    "\n",
    "# # Convert tokens to indices and pad sequences\n",
    "# def text_to_sequence(text, vocab, n_tokens):\n",
    "#     tokens = tokenize(text)\n",
    "#     sequence = [vocab.get(token, 0) for token in tokens]  # Convert to indices; use 0 for OOV tokens\n",
    "#     if len(sequence) < n_tokens:\n",
    "#         sequence = sequence + [0] * (n_tokens - len(sequence)) # Pad sequences\n",
    "#     else:\n",
    "#         sequence = sequence[:n_tokens]  # Truncate sequences\n",
    "#     return sequence\n",
    "\n",
    "# # Apply to datasets\n",
    "# n_tokens = 150\n",
    "# train_df['concatenated_num'] = train_df['concatenated'].apply(lambda x: text_to_sequence(x, vocab, n_tokens))\n",
    "# val_df['concatenated_num'] = val_df['concatenated'].apply(lambda x: text_to_sequence(x, vocab, n_tokens))\n",
    "# test_df['concatenated_num'] = test_df['concatenated'].apply(lambda x: text_to_sequence(x, vocab, n_tokens))\n",
    "\n",
    "# # save vocab and data\n",
    "# save_dir = os.path.join(raw_data_dir, '8_natural_language_inference')\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "# save_path_vocab = os.path.join(save_dir, 'vocab.pkl')\n",
    "# save_path_train = os.path.join(save_dir, 'df_train.pkl')\n",
    "# save_path_val = os.path.join(save_dir, 'df_val.pkl')\n",
    "# save_path_test = os.path.join(save_dir, 'df_test.pkl')\n",
    "\n",
    "# with open(save_path_vocab, \"wb\") as file:\n",
    "#     pickle.dump(vocab, file)\n",
    "# train_df.to_pickle(save_path_train)\n",
    "# val_df.to_pickle(save_path_val)\n",
    "# test_df.to_pickle(save_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To directly load the dataframes with the mapped tokens, run the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary and dataframes from the file\n",
    "save_dir = os.path.join(raw_data_dir, '8_natural_language_inference')\n",
    "save_path_vocab = os.path.join(save_dir, 'vocab.pkl')\n",
    "save_path_train = os.path.join(save_dir, 'df_train.pkl')\n",
    "save_path_val = os.path.join(save_dir, 'df_val.pkl')\n",
    "save_path_test = os.path.join(save_dir, 'df_test.pkl')\n",
    "\n",
    "with open(save_path_vocab, \"rb\") as file:\n",
    "    vocab = pickle.load(file)\n",
    "train_df = pd.read_pickle(save_path_train).reset_index(drop=True)\n",
    "val_df = pd.read_pickle(save_path_val).reset_index(drop=True)\n",
    "test_df = pd.read_pickle(save_path_test).reset_index(drop=True)\n",
    "\n",
    "# look at first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an overview about the dataset:\n",
    "* Are the number of labels for the different classes roughly equal for the training, validation and test set?\n",
    "* Take a look at the first row of the training dataset. Do the numerical sequences generated by the mapping make sense when comparing them to the actual sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# explore category distribution\n",
    "print(train_df['label_text'].value_counts())\n",
    "print('\\n')\n",
    "print(val_df['label_text'].value_counts())\n",
    "print('\\n')\n",
    "print(test_df['label_text'].value_counts())\n",
    "\n",
    "# distribution is roughly equal in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# qualitatively look at first row of training dataset and compare numerical sequence with text\n",
    "print(f\"sentence 1: {train_df.loc[0, 'sentence1']}\")\n",
    "print(f\"sentence 2: {train_df.loc[0, 'sentence2']}\")\n",
    "print(f\"concatenated sentences: {train_df.loc[0, 'concatenated']}\")\n",
    "print(f\"concatenated sentences (numerical): {train_df.loc[0, 'concatenated_num']}\")\n",
    "print(f\"label: {train_df.loc[0, 'label_text']}\")\n",
    "\n",
    "# The mapping seems to have been generated starting from the first row of the training dataframe since the numerical sequence starts with '1, 2, 3'. \n",
    "# Once a word repeats (e.g. the fourth word is again 'a'), the numerical sequence repeats the same number as before (in this case '1').\n",
    "# So everything seems to have worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a PyTorch `Dataset`:\n",
    "* It should take the dataframe as input in the constructor and save it as an attribute. \n",
    "* The ``getitem`` method should index the dataframe and return a tuple consisting of (1) the numerical sequence (``concatenated_num``) and (2) the numerical label (``label_numeric``). Both should be returned as tensors with dtype ``torch.long``. \n",
    "* Check if they return the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    ######### YOUR CODE HERE:\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.dataframe.iloc[idx]['concatenated_num'], dtype=torch.long)\n",
    "        label = torch.tensor(self.dataframe.iloc[idx]['label_numeric'], dtype=torch.long)\n",
    "        return sequence, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NLIDataset(train_df)\n",
    "val_dataset = NLIDataset(val_df)\n",
    "test_dataset = NLIDataset(test_df)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the dataset\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the Transformer model for natural language inference. If you want to first understand the transformer in more detail, there is this great tutorial online: https://jalammar.github.io/illustrated-transformer\\\n",
    "Since Pytorch provides readily usable transformer modules, you will not need to implement the attention mechanism yourself. Still, many of the building blocks that are required are new for you. So make sure to read the documentation to find out how to use them. The model consists of the following building blocks:\n",
    "\n",
    "1. **Embedding Layer**: The input sequences from the dataloader are just integers. In the transformer model, each token is represented by a high-dimensional vector since it has a stronger representational capacity. Use the module ``nn.Embedding`` to map the integer tokens onto vectors of size **embed_dim**. When instantiating ``nn.Embedding``, the size of the vocabulary (**vocab_size**) is also required. The module needs to know the size of the vocabulary because it must learn for every index in the vocabulary a corresponding high-dimensional vector.\n",
    "2. **Positional Encoding**: When you look at the attention formulas (see e.g. linked tutorial), you will notice that the position of the tokens is not used in the calculation of the attention. The transformer is in principle agonistic to the ordering of tokens! That means we have to explicitly encode the position of tokens before we pass them to the transformer if we want the model to use this information. For the natural language inference task, the order of tokens is crucial. Feel free to verify this in an experiment :)! The easiest way to encode the position is to use a learned positional encoding (use ``nn.Parameter``) and simply add it to the embeddings elementwise. We know that we always have **n_tokens** tokens since we pad with zeros. That means the learned positional encoding should have the size **n_tokens** x **embed_dim**. We can add this positional encoding to the embeddings which also have the size **n_tokens** x **embed_dim**.\n",
    "3. **Transformer Encoder**: To define the individual layers in the transformer encoder, use ``nn.TransformerEncoderLayer``. Provide the arguments **embed_dim**, **num_heads**, **dim_feedforward** and set batch_first=True. The individual layers can be combined using ``nn.TransformerEncoder``. ``nn.TransformerEncoder`` takes as arguments the previously defined ``nn.TransformerEncoderLayer`` and the number of layers (**num_layers**).\n",
    "4. **Final Classification Layer**: After the tokens have aggregated information through the transformer encoder, the output will still have size **n_tokens** x **embed_dim**. For the final classification, take the average across tokens (resulting in a tensor of size **embed_dim**) and map the resulting tensor of size **embed_dim** to three classes using a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use learned positional encoding here, but using fixed positional encoding leads to similar results\n",
    "class TransformerNLIModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, n_tokens):\n",
    "        super().__init__()\n",
    "        ######### YOUR CODE HERE:\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, n_tokens, embed_dim))\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 3)\n",
    "        \n",
    "    def forward(self, x, embedded=False, use_pos_encoder=True):\n",
    "        # If x is already an embedding, we don't pass it through the embedding layer (this was not specified in the task, but will be needed later)\n",
    "        if not embedded:\n",
    "            x = self.embedding(x) * np.sqrt(x.size(-1))\n",
    "        \n",
    "        # If use_pos_encoder is False, we don't use positional encoding (this was not specified in the task, but could be useful for comparison studies)\n",
    "        if use_pos_encoder:\n",
    "            x = x + self.pos_encoding\n",
    "        \n",
    "        output = self.transformer_encoder(x)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# # this fixed positional encoding could be used instead of the learned positional encoding\n",
    "# class PositionalEncodingFixed(nn.Module):\n",
    "#     def __init__(self, embed_dim, n_tokens):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(n_tokens, embed_dim)\n",
    "#         position = torch.arange(0, n_tokens, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)  # (1, n_tokens, embed_dim)\n",
    "#         self.register_buffer('pe', pe)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and evaluation functions are very similar to the previous exercises, so they are just provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for sequences, labels in tqdm(train_loader):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds).argmax(1)\n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(train_loader), train_accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tqdm(val_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds).argmax(1)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return total_loss / len(val_loader), val_accuracy, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time limitations, we will just train the model for 5 epochs (~20 mins). Performance will be much better when training longer! But you should still achieve roughly 65\\% accuracy within 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model\n",
    "vocab_size = len(vocab) + 1  # Teh vocabulary does not include the padding index (0)! So we need to add 1 to the vocab size\n",
    "embed_dim = 100\n",
    "num_heads = 10\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "n_tokens = 150\n",
    "\n",
    "model = TransformerNLIModel(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, n_tokens)\n",
    "model = model.to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# save best model state dict\n",
    "save_dir_state_dict = os.path.join(output_data_dir, '8_natural_language_inference')\n",
    "os.makedirs(save_dir_state_dict, exist_ok=True)\n",
    "save_path_state_dict = os.path.join(save_dir_state_dict, 'best.pth')\n",
    "save_path_metrics = os.path.join(save_dir_state_dict, 'metrics.pkl')\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_accuracy, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), save_path_state_dict)\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'train_accuracy': train_accuracies,\n",
    "        'val_accuracy': val_accuracies\n",
    "    })\n",
    "    metrics.to_pickle(save_path_metrics)\n",
    "    \n",
    "print(f'Best Validation Accuracy: {best_val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)\n",
    "\n",
    "####################### plot losses\n",
    "n = len(results)\n",
    "plt.plot(np.linspace(1, n, n), results['train_loss'], c='blue', label='Training Loss')\n",
    "plt.plot(np.linspace(1, n, n), results['val_loss'], c='red', label='Validation Loss')\n",
    "\n",
    "# Mark the minimum validation loss\n",
    "index = np.argmin(results['val_loss'])\n",
    "plt.plot(index+1, results['val_loss'][index], 'kx', label='Min Validation Loss', markersize=12)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(save_path_metrics)\n",
    "\n",
    "####################### plot accuracies\n",
    "n = len(results)\n",
    "plt.plot(np.linspace(1, n, n), results['train_accuracy'], c='blue', label='Training accuracy')\n",
    "plt.plot(np.linspace(1, n, n), results['val_accuracy'], c='red', label='Validation accuracy')\n",
    "\n",
    "# Mark the maximum validation accuracy\n",
    "index = np.argmax(results['val_accuracy'])\n",
    "plt.plot(index+1, results['val_accuracy'][index], 'kx', label='max Validation accuracy')\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print maximum accuracy\n",
    "print(f\"maximum validation accuracy: {np.max(results['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### evaluate test performance\n",
    "# Load the best model\n",
    "vocab_size = len(vocab) + 1  # Teh vocabulary does not include the padding index (0)! So we need to add 1 to the vocab size\n",
    "embed_dim = 100\n",
    "num_heads = 10\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "n_tokens = 150\n",
    "\n",
    "model = TransformerNLIModel(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, n_tokens)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(save_path_state_dict))\n",
    "\n",
    "# get test loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "test_loss, test_accuracy, all_labels, all_preds = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ``all_labels`` and ``all_preds``, plot a confusion matrix (see also exercise 4_convnet). Are all classes equally hard to predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# Compute the confusion matrix\n",
    "classes = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Display the confusion matrix with class names\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# It is visible that the model does not easily confuse the classes \"entailment\" and \"contradiction\", probably because the difference between the sentences is often quite large.\n",
    "# The \"neutral\" class has a higher error rate than the other classes. Apparently, the distinction to both other classes is less clear than the one between \"entailment\" and \"contradiction\".\n",
    "# Further analyses would be necessary to determine why exactly this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability via integrated gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will not have to write any code yourself. The goal is to simply introduce an explainability method for deep learning methods, namely the so-called \"integrated gradients\" method. Intuitively, this method computes the gradient of the output with respect to the model input to determine which features (in our case tokens) are the most important. These features will have the highest gradient. We can use this method to visualize token importance. The method can also be applied to images or other types of inputs and offers a nice way to add explainability to methods. To run the code below, you need to copy the ``TransformerNLIModel`` from the solution here since a small modification is needed for integrated gradients. We will also take a model checkpoint that was (1) trained for 30 epochs and (2) with pre-trained token embeddings (more on that in the next section). The results are much better (82\\% test accuracy) such that attributions will be clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reverse vocabulary where keys are indices and values are words\n",
    "reverse_vocab = {value: key for key, value in vocab.items()}\n",
    "assert len(vocab) == len(reverse_vocab)\n",
    "reverse_vocab[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the IntegratedGradients class from the captum library to compute the attributions with respect to a \"target\". \n",
    "# In our case we have three outputs since we have three classes and we can compute the attributions with respect to either of the output channels. This can be specified by \"target\".\n",
    "def get_attributions(input, target):\n",
    "    # forward function to use for computing integrated gradients\n",
    "    def forward_func(embeddings):\n",
    "        return model(embeddings, embedded=True)\n",
    "\n",
    "    # Create the IntegratedGradients instance\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    # manually perform first step of the model (embedding layer) since integrated gradients does not work with the original inputs since they are just indices\n",
    "    embeddings = model.embedding(input) * np.sqrt(input.size(-1))\n",
    "\n",
    "    # Compute attributions with respect to the embeddings for the target class (we will input the predicted class to visualize what caused the prediction)\n",
    "    attributions = ig.attribute(inputs=embeddings, target=target)\n",
    "    return attributions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the best model\n",
    "vocab_size = len(vocab) + 1  # Teh vocabulary does not include the padding index (0)! So we need to add 1 to the vocab size\n",
    "embed_dim = 100\n",
    "num_heads = 10\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "n_tokens = 150\n",
    "\n",
    "model = TransformerNLIModel(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, n_tokens)\n",
    "model = model.to(device)\n",
    "save_path_pretrained_state_dict = os.path.join(raw_data_dir, '8_natural_language_inference/best_pretrained.pth')\n",
    "model.load_state_dict(torch.load(save_path_pretrained_state_dict))\n",
    "\n",
    "# get prediction, labels, text and corresponding attributions for the first 100 samples in the test dataset\n",
    "tokenized_texts = []\n",
    "labels_numeric = []\n",
    "labels_text = []\n",
    "all_attributions = []\n",
    "predictions = []\n",
    "for _, row in tqdm(test_df.iterrows()):\n",
    "    x = row['concatenated_num']\n",
    "    x = torch.tensor(x, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # tokenized text\n",
    "    tokenized_text = [reverse_vocab[index.item()] for index in x[0]]\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "    \n",
    "    # labels\n",
    "    label_numeric = row['label_numeric']\n",
    "    labels_numeric.append(label_numeric)\n",
    "    label_text = row['label_text']\n",
    "    labels_text.append(label_text)\n",
    "    \n",
    "    # prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x, embedded=False)\n",
    "    prediction = output.argmax(1).item()\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # attributions\n",
    "    attributions = get_attributions(x, prediction)\n",
    "    all_attributions.append(attributions)\n",
    "    \n",
    "    if _ == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the predictions. Note that we always computed the attributions for the *predicted class*. A high attribution value of a token means that it had a positive impact for making this prediction. For example, let's look at the 100th prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 100\n",
    "print(f'text label: {labels_text[ind]}')\n",
    "print(f'numeric label: {labels_numeric[ind]}')\n",
    "print(f'numeric prediction: {predictions[ind]}')\n",
    "print('\\n')\n",
    "print(test_df.iloc[ind]['sentence1'])\n",
    "print(test_df.iloc[ind]['sentence2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is correct. Let's see what had the highest influence in making this prediction. For this we color the average token attribution. *Dark red* means that these tokens had a negative influence on the prediction, while *dark green* means the influence was positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean attribution score across all dimensions of the embeddings to get overall importance of each token\n",
    "mean_attributions = np.mean(all_attributions[ind].squeeze(), axis=1)  # Replace with your actual attributions\n",
    "\n",
    "# Filter out padding tokens\n",
    "filtered_tokens = [token for token in tokenized_texts[ind] if token != \"<PAD>\"]\n",
    "filtered_attributions = mean_attributions[:len(filtered_tokens)]\n",
    "\n",
    "# Normalize the attributions scores to values between 0 and 1 for coloring\n",
    "norm = Normalize(vmin=min(filtered_attributions), vmax=max(filtered_attributions))\n",
    "cmap = matplotlib.colormaps.get_cmap(\"RdYlGn\")  # Red (negative) to green (positive)\n",
    "\n",
    "# Create HTML for colored text visualization\n",
    "colored_text = \"\"\n",
    "for token, score in zip(filtered_tokens, filtered_attributions):\n",
    "    color = cmap(norm(score))  # Get color from normalized score\n",
    "    color_hex = matplotlib.colors.rgb2hex(color[:3])  # Convert RGB to hex\n",
    "    colored_text += f'<span style=\"color: {color_hex}\">{token}</span> '\n",
    "\n",
    "# Display the colored text as HTML\n",
    "display(HTML(f\"<p style='font-size: 16px;'>{colored_text}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word *woman* had a high positive influence on the *contradition* prediction. Apparently, the model focused on the contrast between *man* and *woman* in the two sentences to conclude that there is a contradiction. Obviously, these attributions are not that easily interpretable. For example, the presence of *man* is just as important in determining the contradiction even though it is colored in red. Nevertheless, the attributions (also the magnitude) give a rough idea of what is important for the model, but admittedly it might not be the perfect method for the task at hand. Feel free to look at further examples and try to identify patterns :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we trained our transformer, we trained the *embeddings* for each token from scratch. In practice this is rarely done since embeddings that have been obtained by training on huge language corpora are readily available. In this section, we will explore such pretrained embeddings and investigate whether closeness between embeddings relates to semantic closeness. First, we will load the pretrained GloVe embeddings (Global Vectors for Word Representation) and create a vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = os.path.join(raw_data_dir, '8_natural_language_inference/glove.6B.100d.txt')\n",
    "\n",
    "# Open the GloVe file and read line by line. Obtain all words and embeddings\n",
    "reverse_vocab = dict()\n",
    "all_embeddings = []\n",
    "with open(glove_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        values = line.split()\n",
    "        # get tokens\n",
    "        word = values[0]\n",
    "        reverse_vocab[i] = word\n",
    "        # get embeddings\n",
    "        embedding = torch.tensor([float(val) for val in values[1:]], dtype=torch.float32).unsqueeze(0)\n",
    "        all_embeddings.append(embedding)\n",
    "\n",
    "# get all embeddings as tensor and create vocab with swapped keys and values\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "vocab = {value: key for key, value in reverse_vocab.items()}\n",
    "assert len(vocab) == len(reverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print an example token and the corresponding embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(f'example token: {reverse_vocab[index]}')\n",
    "print(f'corresponding embedding: {all_embeddings[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to measure similarity between vectors is the cosine similarity. For two vectors A and B, the cosine similarity is given as:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{A \\cdot B}{|A| |B|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "The cosine similarity takes on values between -1 and 1. A value of 1 means that the vectors are identical and -1 that they are diametrically opposing. A nice visualization of the concept can be found at: https://www.learndatasci.com/glossary/cosine-similarity/\n",
    "\n",
    "#### Example:\n",
    "Let’s consider two 2D vectors:\n",
    "\n",
    "A = (1, 1) \\\n",
    "B = (1, 0.5)\n",
    "\n",
    "First, calculate the dot product \\( A $\\cdot$ B \\):\n",
    "\n",
    "$$\n",
    "A \\cdot B = (1 \\cdot 1) + (1 \\cdot 0.5) = 1 + 0.5 = 1.5\n",
    "$$\n",
    "\n",
    "Now, calculate the magnitudes |A| and |B|:\n",
    "\n",
    "$$\n",
    "|A| = \\sqrt{(1^2 + 1^2)} = \\sqrt{1 + 1} = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "|B| = \\sqrt{(1^2 + 0.5^2)} = \\sqrt{1 + 0.25} = \\sqrt{1.25}\n",
    "$$\n",
    "\n",
    "Now, plug these values into the cosine similarity formula:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{A \\cdot B}{|A| |B|} = \\frac{1.5}{\\sqrt{2} \\cdot \\sqrt{1.25}}\n",
    "$$\n",
    "\n",
    "Simplifying further:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{1.5}{\\sqrt{2.5}} = \\frac{1.5}{1.5811} \\approx 0.9487\n",
    "$$\n",
    "\n",
    "Thus, the cosine similarity between vectors A = (1, 1) and B = (1, 0.5) is approximately 0.9487. This indicates a high similarity, as the cosine similarity value is close to 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your task:\n",
    "\n",
    " write a function that calculates the cosine similarity between a single embedding (a tensor of size 100) and all GloVe embeddings (a tensor of size 400000 x 100). You can do so by looping through the GloVe embeddings and computing the cosine similarity one by one using the formula above.\n",
    " * ``torch.dot`` can be used to compute the dot product\n",
    " * ``torch.linalg.norm`` can be used to compute the magnitude of a vector.\n",
    " * In the end, all individual cosine similarities should be concatenated to a form a tensor of size 400000 that contains the cosine similarities to all embeddings\n",
    "\n",
    "Alternatively, you do not have to loop. You can also achieve everything at once leveraging broadcasting and matrix multiplication. In practice, this would be recommended as it is orders of magnitude faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity without for loop\n",
    "# def compute_cosine_similarity(single_embedding, all_embeddings):\n",
    "#     # Compute the cosine similarity between the embedding and all other embeddings\n",
    "#     dot_product = all_embeddings @ single_embedding.unsqueeze(1)\n",
    "#     dot_product = dot_product.squeeze()\n",
    "#     magnitude_single_embedding = torch.linalg.norm(single_embedding)\n",
    "#     magnitude_all_embeddings = torch.linalg.norm(all_embeddings, dim=1)\n",
    "#     product_magnitudes = magnitude_all_embeddings * magnitude_single_embedding\n",
    "#     cosine_similarity = dot_product / product_magnitudes\n",
    "#     return cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity(single_embedding, all_embeddings):\n",
    "    ######### YOUR CODE HERE:\n",
    "    # Loop through each embedding in all_embeddings\n",
    "    cosine_similarities = []\n",
    "    for embedding in tqdm(all_embeddings):\n",
    "        # dot product\n",
    "        dot_product = torch.dot(embedding, single_embedding)\n",
    "        \n",
    "        # magnitudes\n",
    "        magnitude_single_embedding = torch.linalg.norm(single_embedding)\n",
    "        magnitude_embedding = torch.linalg.norm(embedding)\n",
    "        \n",
    "        cosine_similarity = dot_product / (magnitude_single_embedding * magnitude_embedding)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "    \n",
    "    # Concatenate all cosine similarities into a single tensor\n",
    "    cosine_similarities = torch.stack(cosine_similarities)\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the cosine similarity of a specific token with all other tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'love'\n",
    "index = vocab[token]\n",
    "\n",
    "# compute cosine similarity\n",
    "single_embeddings = all_embeddings[index]\n",
    "cosine_similarity = compute_cosine_similarity(single_embeddings, all_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's write some code that retrieves the most similar tokens. Proceed as follows:\n",
    "1. Obtain the indices of, say, the 5 most similar tokens\n",
    "2. The corresponding token names can then be retrieved using the ``reverse_vocab`` object that we defined above\n",
    "\n",
    "Do similarities between embeddings correspond to semantic similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "# code for getting the most similar tokens\n",
    "similarities_argsort = cosine_similarity.argsort(descending=True)\n",
    "ind_highest_similarities = similarities_argsort[:5]\n",
    "\n",
    "for i in ind_highest_similarities:\n",
    "    print(f'Token with highest similarity to \"{token}\": {reverse_vocab[i.item()]}')\n",
    "    print(f'corresponding similarity: {cosine_similarity[i]:.4f}')\n",
    "    print('\\n')\n",
    "\n",
    "# The most similar token to 'love' is 'me'. Maybe a little bit surprising that it is not 'you'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you got to know many new concepts. Further ideas you could pursue are the following:\n",
    "* Use the integrated gradients method to visualize attributions in some of the previous exercises. For example, it nicely works for image classification or weight regression.\n",
    "* Try training the transformer without a positional embedding. Do you think the performance will decrease? Why could the positional embedding be especially important for the natural language inference task?\n",
    "* Try initializing the embeddings of the transformer with the pretrained embeddings from GloVe and see if it improves performance! (code provided in the solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to why positional encoding is important**: In general, sentence understanding is hard without positional encoding. Furthermore, the way we tackle the natural language inference task makes positional encodings especially important. We concatenate the two sentences with a \\<sep\\> token in between. All of this structure is lost when we do not provide a positional encoding. Thus, it can be expected that performance is much worse without positional encoding. This can also be verified in experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE:\n",
    "######################################################## Code for initializing model embeddings with GloVe embeddings\n",
    "############################ Instantiate model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the best model\n",
    "vocab_size = len(vocab) + 1  # Teh vocabulary does not include the padding index (0)! So we need to add 1 to the vocab size\n",
    "embed_dim = 100\n",
    "num_heads = 10\n",
    "num_layers = 3\n",
    "dim_feedforward = 512\n",
    "n_tokens = 150\n",
    "\n",
    "model = TransformerNLIModel(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, n_tokens)\n",
    "\n",
    "############################ load GloVe embeddings\n",
    "glove_file_path = os.path.join(raw_data_dir, '8_natural_language_inference/glove.6B.100d.txt')\n",
    "\n",
    "# Open the GloVe file and read it line by line to obtain for every token the corresponding embedding\n",
    "glove_embeddings = {}\n",
    "with open(glove_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float32)\n",
    "        # store token and tensor in the dictionary\n",
    "        glove_embeddings[token] = vector\n",
    "\n",
    "############################ replace randomly initialized embeddings with pretrained embeddings if they exist\n",
    "# load vocabulary that we obtained from the training data of the natural language inference task\n",
    "with open(save_path_vocab, \"rb\") as file:\n",
    "    vocab = pickle.load(file)\n",
    "    \n",
    "n_unknown_tokens = 0\n",
    "with torch.no_grad():\n",
    "    for key in vocab:\n",
    "        index = vocab[key]\n",
    "        if key in glove_embeddings:\n",
    "            model.embedding.weight[index] = glove_embeddings[key]\n",
    "        else:\n",
    "            n_unknown_tokens += 1\n",
    "            model.embedding.weight[index] = torch.zeros(embed_dim)\n",
    "n_unknown_tokens\n",
    "\n",
    "# training this model leads to faster convergence and better performance due to the initilaization with semantically meaningful embeddings. Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further reads\n",
    "\n",
    "* natural language inference benchmark paper: https://arxiv.org/pdf/1508.05326\n",
    "* transformer paper: https://arxiv.org/pdf/1706.03762\n",
    "* integrated gradients paper: https://arxiv.org/pdf/1703.01365"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
