{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a tensor of size 100 with random numbers, obtain a new tensor that contains the 10 largest elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of random numbers\n",
    "torch.manual_seed(1729)\n",
    "r = torch.rand(100)\n",
    "print(r)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "# using sort\n",
    "sorted_r = torch.sort(r, descending=True)\n",
    "largest_r = sorted_r[0][:10]\n",
    "print(largest_r)\n",
    "\n",
    "# using argsort\n",
    "inds = torch.argsort(r, descending=True)\n",
    "largest_r = r[inds[:10]]\n",
    "print(largest_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given a tensor of size 64x100, obtain for each row the index of the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor of random numbers\n",
    "torch.manual_seed(1729)\n",
    "r = torch.rand(64, 100)\n",
    "print(r)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "# using max\n",
    "row_max = torch.max(r, dim=1)[0]\n",
    "print(row_max)\n",
    "\n",
    "# using argmax\n",
    "inds = torch.argmax(r, dim=1)\n",
    "row_max = r[torch.arange(64), inds]\n",
    "print(row_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replicate the result from the broadcasting example from the Pytorch dimensions section without broadcasting. \\\n",
    "Hint: You can use the torch.tile function to obtain a tensor with the same size of the image and perform elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a pseudo-image first\n",
    "image = torch.randn(3, 224, 224)\n",
    "print(f'The shape of the image tensor: {image.shape}')\n",
    "# we want to multiply the red channel by 0, the green channel by 0, and the blue channel by 1\n",
    "multiplier = torch.tensor([0, 0, 1])\n",
    "# We add dimensions to the multiplier tensor to make it compatible with the image tensor\n",
    "multiplier = multiplier[:, None, None]\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "multiplier = torch.tile(multiplier, [1, 224, 224])\n",
    "print(f'\\nThe shape of the multiplier tensor: {multiplier.shape}')\n",
    "product = image * multiplier\n",
    "# All red and green values will be zeros\n",
    "print('\\nThe red and green values are zero:')\n",
    "print(product[[0, 1], :, :])\n",
    "# All blue values will be the same as in the original image\n",
    "print('\\nThe blue values are the same as in the original image:')\n",
    "print(product[2, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Given a tensor of size 10, obtain a new tensor of size 100 where each element in the original tensor is repeated 10 times. \\\n",
    "This is different from repeating the full tensor 10 times which could be done with torch.tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.arange(10)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "r_repeated = torch.repeat_interleave(r, 10)\n",
    "print(r_repeated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What does the torch.flatten function do? Apply the function to a batch of pseudo images. \\\n",
    "Only flatten the images and not the batch dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.rand(64, 3, 224, 224)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "images_flattened = torch.flatten(images, start_dim=1)\n",
    "print(images_flattened.shape)\n",
    "# torch.flatten is a function that flattens a tensor, meaning that it removes all dimensions except for one.\n",
    "# The values from multiple dimensions are concatenated into a single dimension.\n",
    "# In this case this new dimension has a size of 3*224*224=150528.\n",
    "# The start_dim parameter specifies the first dimension that should be flattened.\n",
    "# In this case, the first dimension is not flattened, so the batch size is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define and apply a linear layer with 5 in_features and 2 out_features. Apply the layer and inspect the output shape and values. \\\n",
    "Obtain the same output shape by manually defining a (random) weights matrix and multiplying the inputs with it using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(8, 5)\n",
    "print(inputs)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "linear_layer = nn.Linear(in_features=5, out_features=2)\n",
    "result = linear_layer(inputs)\n",
    "print('\\n')\n",
    "print(result.shape)\n",
    "print(result)\n",
    "\n",
    "weights = torch.randn(5, 2)\n",
    "result = torch.matmul(inputs, weights)\n",
    "print('\\n')\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What does the nn.ReLu module do? Apply the module on a random tensor and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.rand(100)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "relu = nn.ReLU()\n",
    "relu(r)\n",
    "# The relu module is a function sets all negative values to zero and leaves positive values unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. You already applied a linear layer with a specific number of input and output features to a batch of inputs in exercise 5. \\\n",
    "Now your task is to alternately apply mutliple linear layers and relu functions successively to a batch of inputs. \\\n",
    "The three linear layers should have the following input/output feature sizes: 5/10, 10/20 and 20/1. \\\n",
    "Each linear layer except for the last one should be followed by a relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(8, 5)\n",
    "print(inputs.shape)\n",
    "\n",
    "\n",
    "######### YOUR CODE HERE:\n",
    "# apply modules one by one:\n",
    "linear1 = nn.Linear(5, 10)\n",
    "linear2 = nn.Linear(10, 20)\n",
    "linear3 = nn.Linear(20, 1)\n",
    "relu1 = nn.ReLU()\n",
    "relu2 = nn.ReLU()\n",
    "\n",
    "x = linear1(inputs)\n",
    "x = relu1(x)\n",
    "x = linear2(x)\n",
    "x = relu2(x)\n",
    "outputs = linear3(x)\n",
    "print(outputs.shape)\n",
    "\n",
    "# you can also use nn.ModuleList to apply the modules in a loop:\n",
    "module_list = nn.ModuleList([linear1, relu1, linear2, relu2, linear3])\n",
    "x = inputs\n",
    "for module in module_list:\n",
    "    x = module(x)\n",
    "print(x.shape)\n",
    "\n",
    "# you can also use the nn.Sequential module to directly apply the modules in a sequence:\n",
    "# It automatically applies the modules in the order they are passed:\n",
    "# The star operator (*) can be used to unpack a list and pass its elements as arguments to a function.\n",
    "modules = nn.Sequential(*module_list)\n",
    "outputs = modules(inputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Bonus: We did not yet talk about why it makes sense to stack multiple linears and relu functions behind each other. \\\n",
    "So do not worry about this in too much detail now. However, to provide some food for thought: \\\n",
    "Do you have an idea why it does NOT make sense to simply stack multiple linear layers behind each other? \\\n",
    "How is this counteracted by adding relu functions? \\\n",
    "Justify your answer in words (or a small mathematical proof)\n",
    "\n",
    "Hint: Consider the representational capacity of multiple linear layers stacked behind each other.\n",
    "\n",
    "\n",
    "\n",
    "######### YOUR ANSWER HERE: \n",
    "\n",
    "The representational capacity of multiple linear layers stacked behind each other does not increase compared to one single linear layer! Consider a minimal example for this. The input is two dimensional (x1, x2) and we apply two linear layers to it which each have 2/2 as the number of input and output features. For simplicity, we omit the bias again. We denote the outputs of the first and second linear layer with (h1, h2) and (h3, h4) respectively:\n",
    "\n",
    "first layer outputs: \\\n",
    "h1 = w1_1 * x1 + w1_2 * x2 \\\n",
    "h2 = w2_1 * x1 + w2_2 * x2\n",
    "\n",
    "second layer outputs (takes result of first layer as input): \\\n",
    "h3 = w3_1 * h1 + w3_2 * h2 \\\n",
    "h4 = w4_1 * h1 + w4_2 * h2\n",
    "\n",
    "We can now plug in the definition of h1 and h2 from the first layer output: \\\n",
    "h3 = w3_1 * (w1_1 * x1 + w1_2 * x2) + w3_2 * (w2_1 * x1 + w2_2 * x2) \\\n",
    "h4 = w4_1 * (w1_1 * x1 + w1_2 * x2) + w4_2 * (w2_1 * x1 + w2_2 * x2)\n",
    "\n",
    "If we multiply everything out, we get: \\\n",
    "h3 = (w3_1 * w1_1 + w3_2 * w2_1) * x1 + (w3_1 * w1_2 + w3_2 * w2_2) * x2 \\\n",
    "h4 = (w4_1 * w1_1 + w4_2 * w2_1) * x1 + (w4_1 * w1_2 + w4_2 * w2_2) * x2\n",
    "\n",
    "Notice that the outputs h3 and h4 are linear combinations of the inputs x1 and x2, where the coefficients are just new weights that are combinations of the original weights. Therefore, the result is still a simple linear transformation of the input:\n",
    "\n",
    "h3 = q1_1 * x1 + q1_2 * x2 \\\n",
    "h4 = q2_1 * x1 + q2_2 * x2\n",
    "\n",
    "where the q's are defined as follows: \\\n",
    "q1_1 = w3_1 * w1_1 + w3_2 * w2_1 \\\n",
    "q1_2 = w3_1 * w1_2 + w3_2 * w2_2 \\\n",
    "q2_1 = w4_1 * w1_1 + w4_2 * w2_1 \\\n",
    "q2_2 = w4_1 * w1_2 + w4_2 * w2_2\n",
    "\n",
    "Conclusion:\n",
    "This implies that stacking multiple linear layers does not increase the representational capacity beyond what a single linear layer can achieve. The reason is that a composition of linear transformations is still a linear transformation. Thus, stacking multiple linear layers without any non-linear activation functions between them is equivalent to a single linear layer.\n",
    "\n",
    "The Role of ReLU:\n",
    "By adding ReLU (or any other non-linear activation function) between the layers, we introduce non-linearity into the model. This non-linearity allows the network to model more complex functions that go beyond simple linear transformations. Specifically, ReLU can \"break\" the linearity by zeroing out negative values, allowing the model to create more complex decision boundaries and learn more intricate patterns in the data (more detail on that tomorrow).\n",
    "\n",
    "Here a small code demonstration of the mini \"proof\" provided above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensor\n",
    "x = torch.tensor([1.0, 2.0])  # Example input (x1 = 1, x2 = 2)\n",
    "\n",
    "# Define the weights for the first linear layer\n",
    "W1 = torch.tensor([[1.0, 2.0],  # w1_1, w1_2\n",
    "                   [3.0, 4.0]])  # w2_1, w2_2\n",
    "\n",
    "# Define the weights for the second linear layer\n",
    "W2 = torch.tensor([[5.0, 6.0],  # w3_1, w3_2\n",
    "                   [7.0, 8.0]])  # w4_1, w4_2\n",
    "\n",
    "# First linear layer\n",
    "h = torch.matmul(W1, x)  # h1, h2\n",
    "\n",
    "# Second linear layer\n",
    "output_stacked = torch.matmul(W2, h)  # h3, h4\n",
    "\n",
    "# Now, let's calculate the equivalent single linear transformation\n",
    "W_combined = torch.matmul(W2, W1)\n",
    "output_single = torch.matmul(W_combined, x)  # Equivalent single linear transformation\n",
    "\n",
    "# Print results\n",
    "print(\"Output of stacked linear layers:\", output_stacked)\n",
    "print(\"Output of single equivalent linear layer:\", output_single)\n",
    "\n",
    "# Check if they are equivalent\n",
    "equivalent = torch.allclose(output_stacked, output_single)\n",
    "print(\"Are the outputs equivalent?\", equivalent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danuma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
